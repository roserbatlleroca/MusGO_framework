Status,Reviewed by another person,Reponsible,Model,Affiliation(s),Architecture,[1] Open Source Code,[2] Training data,[3] Model weights,[4] Code documentation,[5] Training procedure,[6] Evaluation procedure,[7] Research paper,[13] Licensing,[13] Notes,Openness Level,[8] Model Card,[9] Datasheet,[10] Package,[11] User-oriented application,[12] Demo Page,Repository,Release date,Preprint,Further info,Article,Why?,General Comments,[1] Notes,[10] Notes,[2] Notes,[3] Notes,[4] Notes,[5] Notes,[7] Notes,[8] Notes,[9] Notes,[11] Notes,[12] Notes,"[6] Notes ",Ethical Considerations,# stars,App,Dataset,Datasheet,License,Model card,Package,Page,Readme,Weights
Done,Yes,Martín Rocamora,MusicGen,Meta AI,"autoregressive trasnformer, transformer",✓,≃,✓,✓,✓,✓,✓,✓,The code in this repository is released under the MIT license as found in the https://github.com/facebookresearch/audiocraft/blob/main/LICENSE. The models weights in this repository are released under the CC-BY-NC 4.0 license as found in the https://github.com/facebookresearch/audiocraft/blob/main/LICENSE_weights.,94,★,∅,★,★,★,https://github.com/facebookresearch/audiocraft,"June 8, 2023",https://arxiv.org/pdf/2306.05284,https://github.com/facebookresearch/audiocraft,https://proceedings.neurips.cc/paper_files/paper/2023/hash/94b472a1842cd7c56dcb125fb2765fbd-Abstract-Conference.html,open,,Code is provided in https://github.com/facebookresearch/audiocraft/ (Training and inference),"Has two packages: audiocraft package from Meta, and transformers package from Hugging Face. ","“We rely on an internal dataset of 10K high-quality music tracks, and on the ShutterStock and Pond5 music data collections2 with respectively 25K and 365K instrument-only music tracks.” “Note that we do NOT provide any of the datasets used for training MusicGen. We provide a dummy dataset containing just a few examples for illustrative purposes.”",Models are available in the GitHub repository and in Hugging Face.,Training code is available and seem well documented: https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md Warning: Since data is not available is not easy to replicate.,"Training is well documented. https://github.com/facebookresearch/audiocraft/blob/main/docs/TRAINING.md with training pipelines and environment setup described. Note: Hardware use for training is described in the paper. ",Accepted at NeurIPS 2023,"Model card discusses intended use, limitations and biases. https://github.com/facebookresearch/audiocraft/blob/main/model_cards/MUSICGEN_MODEL_CARD.md",Data is copyrighted and not available.,"Demo in Hugging Face spaces, user-oriented gradio demo, and jupyter notebooks. https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md#usage",Supplementary material web page with sound examples. https://ai.honu.io/papers/musicgen/,Evaluation is documented in the paper and the code provides detailed explanation about the implementation of evaluation metrics. https://github.com/facebookresearch/audiocraft/blob/main/docs/METRICS.md Evaluating the model to reproduce the results still may require some effort. Dataset used for evaluation is MusicCaps benchmark.,"Model card discusses intended use, limitations and biases. Except for the access to training data the model is fully open and well documented. The training data is licensed.  ",4,https://huggingface.co/spaces/facebook/MusicGen,,,https://github.com/facebookresearch/audiocraft/blob/main/LICENSE,https://github.com/facebookresearch/audiocraft/blob/main/model_cards/MUSICGEN_MODEL_CARD.md,https://pypi.org/project/audiocraft/,https://ai.honu.io/papers/musicgen/,https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md,https://huggingface.co/facebook/musicgen-large/tree/main
Done,Yes,Martín Rocamora,Stable Audio Open,Stability AI,"diffusion, transformer",✓,✓,✓,✓,✓,✓,≃,≃,"Stability AI Community License
https://huggingface.co/stabilityai/stable-audio-open-1.0/blob/main/LICENSE.md
Not an Open Source Initiate(OSI) approved licences ",88,★,∅,★,∅,★,https://huggingface.co/stabilityai/stable-audio-open-1.0,"July 31, 2024",http://arxiv.org/abs/2407.14358,https://stability.ai/stable-audio,,open,," Code for data processing, training pipeline, and inference is available in the stable-audio-tools repository. Architecture of the mode is specified in the form of a config file.
https://github.com/Stability-AI/stable-audio-tools",The model can be used with: 1. the https://github.com/Stability-AI/stable-audio-tools library and 2. the https://huggingface.co/docs/diffusers/main/en/index library,"Training data can be mapped from Attribution files.https://huggingface.co/stabilityai/stable-audio-open-1.0/blob/main/fma_dataset_attribution2.csv. However, accessibility is subject to accepting Stability AI conditions. ","Weights are available at: https://huggingface.co/stabilityai/stable-audio-open-1.0. However, accessibility is subject to accepting Stability AI conditions. ","Documentation of the code is limited for training replicating the model (not specific config files, but those from Stable Audio could be used). However, I think we can still consider the documentation of the code to be enough.
https://huggingface.co/stabilityai/stable-audio-open-1.0https://github.com/Stability-AI/stable-audio-tools","Described in pre-print. Warning: They say they provide the exact parameters for training in the repository, but I can’t find them. ","Paper is in arxiv but it is not peer-reviewed. ",Model card in HuggingFace: https://huggingface.co/stabilityai/stable-audio-open-1.0,"They provide a .csv file listing each wav from Freesound and FMA. But there is no proper datasheet. It’s a pity they don’t get this start because they are careful about the data they use.    ",I can’t find an official user-oriented application apart from the packages and code examples. But I am sure that the community around the model is building this kind of resource. We should check.,They provide sound examples in https://stability.ai/news/introducing-stable-audio-open,"Described in pre-print. They use stable-audio-metrics for evaluation, which has examples of evaluation pipelines, but the exact code used for evaluation is unavailable. Warning: I don't know if we can get the subset of prompts they use describing instrumental music from the Song Describer Dataset. ","They make sure they not train with copyrighted music. They do a memorization analysis. The provide a list of intended uses, out-of-scope cases, biases and limitations. ",3,,,,,,,,,
Done,Yes,Roser Batlle Roca,JASCO,"The Hebrew University of Jerusalem, Meta AI",flow matching,✓,≃,✓,✓,≃,✓,✓,✓," Code is released under MIT, model weights are released under CC-BY-NC 4.0.",88,★,∅,★,★,★,https://github.com/facebookresearch/audiocraft,"June 16, 2024",https://arxiv.org/pdf/2406.10970,https://pages.cs.huji.ac.il/adiyoss-lab/JASCO/,https://ismir2024program.ismir.net/poster_65.html,popular,,Code is avialble at https://github.com/facebookresearch/audiocraft/blob/main/docs/JASCO.md,Installable through audiocraft pip package,"They describe the datasets used to training their model. However, there is a 10k songs set that is not described (proprietary data). ","Pretrained models are accessible via API ","Code is documented in GitHub repo, including installation requirements. ","Training procedure is described in the research paper, but hardware requirements are not mentioned. ","Accepted at ISMIR 2024. ",https://github.com/facebookresearch/audiocraft/blob/main/model_cards/JASCO_MODEL_CARD.md,,https://github.com/facebookresearch/audiocraft/blob/main/docs/JASCO.md#api,https://pages.cs.huji.ac.il/adiyoss-lab/JASCO/,"Evaluation procedure is described, including evaluation data, metrics (with reference to the implementation) and model performance. ","The authors state that the training data was obtained through agreements with Shutterstock and Pond5, ensuring legal use. They acknowledge that the dataset is primarily Western-style music, which may limit the diversity of the generated music. The authors also recognize the potential impact of generative music models on professional musicians and emphasize the importance of open research to provide equitable access to such tools for both amateur and professional musicians.",4,https://github.com/facebookresearch/audiocraft/blob/main/docs/JASCO.md#api,,,https://github.com/facebookresearch/audiocraft/blob/main/LICENSE,https://github.com/facebookresearch/audiocraft/blob/main/model_cards/JASCO_MODEL_CARD.md,https://pypi.org/project/audiocraft/,https://pages.cs.huji.ac.il/adiyoss-lab/JASCO/,https://github.com/facebookresearch/audiocraft/blob/main/docs/JASCO.md,https://huggingface.co/facebook/jasco-chords-drums-400M/tree/main
Done,Yes,LAURA IBAÑEZ MARTINEZ,GANsynth,Google Magenta,GAN,✓,≃,✓,✓,✓,≃,✓,✓,"System is covered by Apache License. ",88,∅,∅,★,∅,★,https://github.com/magenta/magenta/tree/main/magenta/models/gansynth,"February 23, 2019",https://arxiv.org/pdf/1902.08710,https://magenta.tensorflow.org/gansynth,https://openreview.net/forum?id=H1xQVn09FX,popular,,Source code is available.,Available within the magenta pip package.,"Dataset is available, but the reduced version that they use and the newly created test/train splits are not provided.",Two pretrained checkpoints are provided.,"Codebase is documented, including high-level instructions and configuration files.","Training procedure is fully documented, including hardware requirements and model configuration.",Accepted at ICLR 2019.,Very few details are given of the pretrained checkpoints.,Not available.,Not available.,https://storage.googleapis.com/magentadata/papers/gansynth/index.html,Evaluation metrics are described but exact implementations are not referenced.,"No comment. ",2,,https://magenta.tensorflow.org/datasets/nsynth,,https://github.com/magenta/magenta/blob/main/LICENSE,,https://pypi.org/project/magenta/,https://storage.googleapis.com/magentadata/papers/gansynth/index.html,https://github.com/magenta/magenta/blob/main/magenta/models/gansynth/README.md,https://github.com/magenta/magenta/blob/main/magenta/models/gansynth/README.md
Done,Yes,Roser Batlle Roca,Musika,Johannes Kepler University Linz,GAN,✓,≃,✓,✓,✓,≃,✓,✓,"MIT License. ",88,∅,∅,∅,★,★,https://github.com/marcoppasini/musika,"August 18, 2022",https://arxiv.org/pdf/2208.08706,https://marcoppasini.github.io/musika,https://ismir2022program.ismir.net/poster_74.html,popular,,"Codebase is available and complete. ","Not available. Source code is provided through GitHub, but not as an installable packaged or with version control. ","Training data is described in the research paper. Some of the data used is publicly available: LibriTTS corpus. However, there are some sections of the used data that are not accessible or detailed enough to fully reconstruct the dataset:  South by SouthWest. Also, music coming from http://jamendo.com under “techno” genre, it not detailed. ","Weights are publicly available. ","Codebase is documented, including details on environment and configuration settings. ","Detailed in research paper and proper instructions are given in the model repository. ",Accepted at ISMIR 2022.,"Not available. ","Not available. ","An interface is provided through Gradio. In addition, there is a colab notebook that intends to ease accessibility for non technical users: https://colab.research.google.com/drive/1PowSw3doBURwLE-OTCiWkO8HVbS5paRb",https://marcoppasini.github.io/musika,"FAD is used for evaluation. However, there are details missing on the evaluation procedure. ","No comment. ",2,https://colab.research.google.com/drive/1PowSw3doBURwLE-OTCiWkO8HVbS5paRb,,,https://github.com/marcoppasini/musika/blob/main/LICENSE,https://huggingface.co/musika/musika_techno,,https://marcoppasini.github.io/musika,https://github.com/marcoppasini/musika/blob/main/README.md,https://huggingface.co/musika/musika_techno/tree/main
Done,Yes,Roser Batlle Roca,VampNet,Northwestern University and Descript Inc,"non-autoregressive transformer, transformer",✓,✗,✓,✓,✓,≃,✓,✓,MIT License,81,∅,∅,∅,★,★,https://github.com/hugofloresgarcia/vampnet,"July 12, 2023",https://arxiv.org/abs/2307.04686,https://hugo-does-things.notion.site/VampNet-Music-Generation-via-Masked-Acoustic-Token-Modeling-e37aabd0d5f1493aa42c5711d0764b33?pvs=4,https://archives.ismir.net/ismir2023/paper/000042.pdf,open,,System source code is available and accessible at https://github.com/hugofloresgarcia/vampnet.,,"Dataset is not available nor is completely described. ",Model checkpoints are available https://zenodo.org/records/8136629. The weights for the models are licensed https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ml,"Codebase is documented. ","Described in the article, with multiple details on architecture, hyperparameters, GPU usage",Article peer-reviewed & accepted at ISMIR 2023: https://archives.ismir.net/ismir2023/paper/000042.pdf,,,"Gradio UI is provided in the codebase. ","Untitled (https://www.notion.so/e37aabd0d5f1493aa42c5711d0764b33?pvs=21) ","Evaluation of the model is described in the research paper. However, evaluation data is not provided. ","The authors acknowledge the proprietary nature of the training dataset and highlight efforts to curate a balanced dataset without artist overlap. However, they do not discuss broader ethical implications such as copyright, cultural biases, or the societal impact of generative music.",2,,,,,,,,,
Done,Yes,Roser Batlle Roca,MusicLDM,"University of California San Diego, Mila-Quebec Artificial Intelligence Institute, University of Surrey, LAION",diffusion,≃,≃,✓,✓,✓,✓,✓,≃,Attribution-NonCommercial-ShareAlike 4.0 International,81,★,★,∅,∅,★,https://github.com/RetroCirce/MusicLDM/?tab=readme-ov-file,"October 5, 2023",https://arxiv.org/pdf/2308.01546,https://huggingface.co/docs/diffusers/api/pipelines/musicldm,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10447265,popular,"Trained on Audiostock. Build on  Stable Diffusion and AudioLDM architectures to the music domain: retraining the contrastive language-audio pretraining
model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music  data samples.
Includes broader impact analysis in article preprint. ","System source code is available at GitHub repository & Hugging Face Diffusers. ",,"MusicLDM is trained on the Audiostock dataset, which contains 9000 music tracks for training and 1000 tracks for testing. Dataset is not directly provided, no information about the original sources accessibility or requirements is provided. ",Model checkpoints are available here: https://drive.google.com/drive/folders/15VDVcIgf99YRM5oGXhRxa_Rowl54uWho?usp=sharing,"Description on how to use the code is provided in the GitHub repo and through Hugging Face Diffusers. 
","Training procedure is documented in the research article preprint and in the appendix additional page for the peer-reviewed version: https://musicldm.github.io/appendix/. Authors include specifications on hyperparameters, GPU requirements and model configurations. ",This article has been accepted at ICASSP 2024. The access of such paper is limited to IEEE Xplore access: https://ieeexplore.ieee.org/document/10447265. A preprint is available in arXiv: https://arxiv.org/pdf/2308.01546,"Available at HuggingFace: https://huggingface.co/ucsd-reach/musicldm 
Information about model’s evaluation and limitations is missing and model’s architecture is just mentioned.  ","There is a data card: https://github.com/LAION-AI/audio-dataset/blob/main/data_card/Audiostock.md. However, content is limited to data sources origin and details on how to reproduce the data collection. Details on curation and other considerations, such as consent, limitations and selection strategies are missing. ","API is shut down temporarily. Should be released again soon. ",https://musicldm.github.io/,Evaluation procedure is described in the research paper. Evaluation code is available in the model’s codebase and in this complementary repo: https://github.com/haoheliu/audioldm_eval,"The ""Broader Impact"" section outlines both positive and negative aspects:
• Positive: Enhances creativity, supports cultural preservation, and offers educational tools. Facilitates the entertainment industry by reducing production costs.
• Negative: Highlights risks like artistic job displacement, copyright issues, ethical misuse, and potential cultural homogenization or appropriation.
The authors aim to mitigate plagiarism by incorporating mixup strategies and aligning data augmentation with ethical considerations.",3,,,,,,,,,
Done,Yes,Roser Batlle Roca,Jukebox,OpenAI,"autoregressive trasnformer, transformer, variational autoencoder",✓,≃,✓,✓,✓,≃,≃,≃,"Codebase is licensed under a non-commercial use license by OpenAI. ",75,∅,∅,∅,∅,★,https://github.com/openai/jukebox,"April 30, 2019",https://arxiv.org/pdf/2005.00341,https://openai.com/index/jukebox/,"No peer-reviewed article found. ",open,,Code is available at https://github.com/openai/jukebox,"Not available. Codebase is officially not mantained. ","Training data is partially described in the preprint article. However, relevant information is missing, such as specific sources, and data itself cannot be accessed — dataset is not publicly available. ",Weights are available.,"Codebase is documented. ","Information available between article and GitHub repository.  ","Only preprint of the research paper is available. ","Not available. ","Not available. ",,"A demo/summary page is available: https://openai.com/index/jukebox/
However, sonifications of the examples generations are not available anymore. There exist another site with generation examples, but most of them are not available https://jukebox.openai.com/. In addition, a colab notebook is provided: https://colab.research.google.com/github/openai/jukebox/blob/master/jukebox/Interacting_with_Jukebox.ipynb","Part of the evaluation procedure is documented in the research paper. However, key information is missing, such as the evaluation dataset, objective evaluation metrics and details on the manual evaluation. ","No comment. ",1,,,,,,,,,
Done,Yes,LAURA IBAÑEZ MARTINEZ,RAVE,IRCAM,autoencoder,✓,✗,✓,✓,✓,✓,≃,≃,System covered by an CC BY-NC 4.0 license.,75,★,∅,★,★,★,https://github.com/acids-ircam/RAVE,"November 9, 2021",https://arxiv.org/pdf/2111.05011,https://forum.ircam.fr/projects/detail/rave-vst/,"submitted to ICLR 2022, but rejected: https://openreview.net/forum?id=cdwobSbmsjA",open,,"Source code is available, including data preparation, model architecture, training pipeline and inference.",Pip package is available.,"Training data is internal and just briefly described. ",Several pretrained models are provided.,"Code is thoroughly documented, allowing for understanding and reproducibility of the model.",Training procedure is described.,"Rejected at ICLR 2022. No peer-reviewed version. ",https://acids-ircam.github.io/rave_models_download,Not available.,Real-time implementation of the model and tutorials are provided.,https://anonymous84654.github.io/RAVE_anonymous/,"Evaluation procedure is described, and the implementations used for the evaluation metrics are referenced.","No comment. ",4,,,,,,,,,
Done,Yes,Roser Batlle Roca,Moûsai,"ETH Zürich, IIT Kharagpur, Max Planck Institute",diffusion,≃,≃,✗,≃,✓,≃,✓,✓,"Code is under the MIT license. ",63,∅,∅,★,∅,★,https://github.com/archinetai/audio-diffusion-pytorch,"January 27, 2023",https://arxiv.org/abs/2301.11757,https://diligent-pansy-4cb.notion.site/Music-Generation-with-Diffusion-ebe6e9e528984fa1b226d408f6002d87,,open,,"They provide an audio diffusion library that includes different models. However, the configs shown are indicative and untested, see https://arxiv.org/abs/2301.11757 for the configs used in the paper.",Code belongs to https://github.com/archinetai/audio-diffusion-pytorch library,"How the data is collected and acquired, including licensing issues, is detailed in the research paper. However, the exact list of songs in the dataset nor direct access to the dataset is available. ","In the GitHub repo, authors mention that “no pre-trained models are provided here”. ",Code is partially describe. How to use Moûsai is not straightforward described.,"Training procedure is describe in the article, including hardware requirements and model configuration. ",Accepted at ACL 2024,,,,"Supplementary material is available at Untitled (https://www.notion.so/ebe6e9e528984fa1b226d408f6002d87?pvs=21) ","Evaluation data nor code are specified. ","The paper includes an Ethical Considerations section, addressing potential concerns about copyright, particularly regarding music that may resemble existing works. The authors emphasize that the model is exempt from copyright infringement for research use but suggest incorporating safeguards for broader use. They also highlight the economic impact on musicians, advocating for the model to enhance human creativity rather than replace it, and encourage collaboration between AI researchers and musicians.",2,,,,,,,,,
Done,Yes,LAURA IBAÑEZ MARTINEZ,Music ControlNet,Carnegie Mellon University and Adobe Research,diffusion,✗,✗,✗,✗,✓,≃,✓,✗,Not available.,31,∅,∅,∅,∅,★,,"November 13, 2023",https://arxiv.org/pdf/2311.07069,https://musiccontrolnet.github.io/web/,https://dl.acm.org/doi/10.1109/TASLP.2024.3399026,popular,,No official code repository available.,No package available.,"Very limited description of the data used for training, and no sources provided.",No model weights provided.,No source code available so no documentation.,Training procedure is described in detail in the paper.,A peer-reviewed paper is available and publicly accessible.,No model card available.,No datasheet available.,No user interface nor real-time implementation available.,https://musiccontrolnet.github.io/web/,"Evaluation procedure is described, but details on the specific implementation used for some of the evaluation metrics are not given, limiting the reproducibility of results.","The authors acknowledge that music generation could disrupt traditional norms around music creation and competition. They emphasize the model's goal to enhance creative agency for musicians through improved control methods. Risks discussed include artist imitation without consent, the inclusion of singing voices, and other unforeseen ethical issues. To mitigate these risks, the authors use licensed instrumental music for training and public domain or self-recorded melodies for inference. They also evaluate the model using the MusicCaps dataset, as it is standard in the field.",1,,,,,,,,,
Done,Yes,LAURA IBAÑEZ MARTINEZ,Diff-A-Riff,Sony Computer Science Laboratories Paris and Queen Mary University of London,diffusion,✗,✗,✗,✗,✓,≃,✓,✗,Not available.,31,∅,∅,∅,∅,★,,"June 12, 2024",https://arxiv.org/abs/2406.08384,https://sonycslparis.github.io/diffariff-companion/,https://arxiv.org/abs/2406.08384,popular,,No source code.,Not available.,"Dataset is closed and only briefly described. ",Not provided.,No source code.,Training procedure is properly described.,Accepted at ISMIR 2024,Not available.,Not available.,Not available.,https://sonycslparis.github.io/diffariff-companion/,Missing data and metrics implementations.,"Authors acknowledge the ethical implications of their proposed model at the end of the research article. “Diff-A-Riff has been trained on a dataset that was legally acquired for internal research and development; therefore, neither the data nor the model can be made publicly available.” ",1,,,,,,,,,
Done,Yes,LAURA IBAÑEZ MARTINEZ,DITTO-2,University of California San Diego and Adobe Research,diffusion,✗,✗,✗,✗,≃,≃,✓,✗,Not available.,25,∅,∅,∅,∅,★,,"May 30, 2024",https://arxiv.org/abs/2405.20289,https://ditto-music.github.io/ditto2/,"Should be in the ISMIR website, but you need GD permision: https://ismir2024program.ismir.net/poster_146.html#paper",popular,,No code repository.,Not available.,Training data neither available nor properly described.,Not provided.,No code repository.,Training procedure is partially described.,Accepted at ISMIR 2024,Not available.,Not available.,Not available.,https://ditto-music.github.io/ditto2/,Metrics and data are described but exact implementations are missing.,"No comment. ",1,,,,,,,,,
Done,Yes,Roser Batlle Roca,Noise2Music,Google Research,diffusion,✗,≃,✗,✗,≃,≃,≃,✗,"Not available. ",25,∅,∅,∅,∅,★,"Not available. ","February 8, 2024",https://arxiv.org/pdf/2302.03917,https://google-research.github.io/noise2music/,"Not available. ",popular,"Includes broader impact analysis in demo page & research paper. ","No source codebase is available. ","Not available. ","Training data is described in detail. However, the resulting MuLaMCap dataset resulting from this work is not publicly available.","Model weights are not available. ","No codebase is available. There is no documentation available either. ","The available preprint describes partially the training procedure of the model. While some relevant information is described, such as model configuration and training details, there are key aspects of the training missing, such as hardware requirements, model checkpoints and hyperparameters. ","Preprint is available. No peer-reviewed version of the article has been found. ","Not available. ","Not available. ",,https://google-research.github.io/noise2music/,"Despite the evaluation procedure is well-documented in regards to evaluation data and metrics, the evaluation process lacks some relevant details to ensure replicability. Moreover, evaluation code of the system is not available. ","In the Broader Impact section, the authors acknowledge several ethical concerns related to their work. They highlight the potential biases in the training data that could manifest in the generated music, such as stereotypes or misrepresentations of specific musical genres. The paper also discusses the possibility of misuse of the technology, particularly when the model generates content that directly replicates copyrighted works or produces culturally insensitive outputs. The authors emphasize the importance of duplication checks in the model's generation pipeline to prevent such misuse. Additionally, they caution that music genres are complex and culturally contextual, and the model’s output may not always accurately reflect the diversity within specific genres.",1,,,,,,,,,
Done,Yes,LAURA IBAÑEZ MARTINEZ,MeLoDy,ByteDance,diffusion,✗,✗,✗,✗,≃,≃,✓,✗,Not available.,25,∅,∅,∅,∅,★,,"May 25, 2024",https://arxiv.org/pdf/2305.15719,https://efficient-melody.github.io/#,https://dl.acm.org/doi/10.5555/3666122.3666888,popular,,No source code.,Not provided.,Not provided and only briefly described.,Not provided.,No source code.,Training procedure partially described.,Accepted at NeurIPS 2023: https://dl.acm.org/doi/10.5555/3666122.3666888,Not provided.,Not provided.,Not provided.,https://efficient-melody.github.io/#,Missing information about data and specific implementations.,No comment.,1,,,,,,,,,
Done,Yes,LAURA IBAÑEZ MARTINEZ,MusicLM,Google Research and IRCAM,"autoregressive trasnformer, transformer",✗,≃,✗,✗,≃,≃,≃,✗,Not available.,25,∅,∅,∅,∅,★,,"June 26, 2023",https://arxiv.org/pdf/2301.11325,https://google-research.github.io/seanet/musiclm/examples/,,popular,,Source code not available.,Not available.,Training data is not available nor properly described.,Not provided.,Not available.,"Training procedure is partially documented, missing the hardware requirements.",Peer-reviewed paper not found.,Not available.,Not available.,In theory available to try in AI Test Kitchen but I never managed?,https://google-research.github.io/seanet/musiclm/examples/,"Evaluation data is provided and metrics are described, but exact implementations are not referenced.","The authors include an explicit ethics statement, discussing risks such as biases in training data, which could lead to underrepresentation of certain musical cultures, and cultural appropriation. They also highlight concerns about the misappropriation of creative content, mitigated through memorization tests adapted from text-based models. To reduce risks, the training data comprises licensed music, and no plans are made to release the model until further research addresses these ethical challenges.",1,,,,,,,,,