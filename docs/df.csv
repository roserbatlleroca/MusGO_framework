project.link,project.license,project.notes,org.name,org.link,org.notes,sourcecode.class,sourcecode.link,sourcecode.notes,trainingdata.class,trainingdata.link,trainingdata.notes,modelweights.class,modelweights.link,modelweights.notes,codedoc.class,codedoc.link,codedoc.notes,trainprocedure.class,trainprocedure.link,trainprocedure.notes,evalprocedure.class,evalprocedure.link,evalprocedure.notes,paper.class,paper.link,paper.notes,paper.date,license.class,license.link,license.notes,modelcard.class,modelcard.link,modelcard.notes,datasheet.class,datasheet.link,datasheet.notes,package.class,package.link,package.notes,ux.class,ux.link,ux.notes,suppage.class,suppage.link,suppage.notes,source.file,openness,star_count
https://github.com/Stability-AI/stable-audio-tools,Stability AI Community License https://huggingface.co/stabilityai/stable-audio-open-1.0/blob/main/LICENSE.md. Not an Open Source Initiate (OSI) or RAIL approved licences.,,Stability AI,https://stability.ai/,,open,https://github.com/Stability-AI/stable-audio-tools,"Code for data processing, training pipeline, and inference is available in the stable-audio-tools repository. Architecture of the mode is specified in the form of a config file.",open,https://info.stability.ai/attributions,Training data can be mapped from Attribution files. Dataset attribution files can also be accessed in HuggingFace after accepting Stability AI conditions (https://huggingface.co/stabilityai/stable-audio-open-1.0/blob/main/fma_dataset_attribution2.csv and https://huggingface.co/stabilityai/stable-audio-open-1.0/blob/main/freesound_dataset_attribution2.csv).,open,https://huggingface.co/stabilityai/stable-audio-open-1.0/tree/main,"Weights are available in the HuggingFace repository. However, accessibility is subject to accepting Stability AI conditions.",open,https://github.com/Stability-AI/stable-audio-tools/blob/main/README.md,"Documentation of the code is limited for training replicating the model. Although there are not specific config files for Stable Audio Open, those from Stable Audio could be used.",open,https://arxiv.org/pdf/2407.14358,Described in pre-print.,open,https://arxiv.org/pdf/2407.14358,"They use stable-audio-metrics for evaluation, which has examples of evaluation pipelines.",open,https://ieeexplore.ieee.org/document/10888461,Accepted at ICASSP 2026.,,partial,https://huggingface.co/stabilityai/stable-audio-open-1.0/blob/main/LICENSE.md,Stability AI Community License. Not an Open Source Initiate (OSI) or RAIL approved licences.,star,https://huggingface.co/stabilityai/stable-audio-open-1.0,Model card in HuggingFace.,∅,,,star,https://pypi.org/project/stable-audio-tools/,The model can be used with 1) the https://github.com/Stability-AI/stable-audio-tools library and 2) the https://huggingface.co/docs/diffusers/main/en/index library.,star,https://github.com/Stability-AI/stable-audio-tools/blob/main/stable_audio_tools/interface/gradio.py,Gradio interface in source code.,star,https://stability.ai/news/introducing-stable-audio-open,A complementary site with sound examples and demonstration of the model's capabilities is available.,/projects/stable-audio-open.yaml,95,4
https://github.com/facebookresearch/audiocraft,The code in this repository is released under the MIT license as found in the https://github.com/facebookresearch/audiocraft/blob/main/LICENSE. The models weights in this repository are released under the CC-BY-NC 4.0 license as found in the https://github.com/facebookresearch/audiocraft/blob/main/LICENSE_weights.,,Meta AI,,,open,https://github.com/facebookresearch/audiocraft,Code is available.,partial,,"They describe the datasets used to training their model (licensed data). However, they rely on 10k music tracks that are not described (proprietary data). They do not provide any of the datasets used for training MusicGen. Only a dummy dataset containing just a few examples for illustrative purposes is provided.",open,https://huggingface.co/facebook/musicgen-large/tree/main,Models are available in the GitHub repository and in Hugging Face.,open,https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md,"Codebase is documented in the GitHub repository, including installation requirements.",open,https://github.com/facebookresearch/audiocraft/blob/main/docs/TRAINING.md,"Training is well documented, with training pipelines and environment setup described. Note that hardware use for training is described in the paper.",open,https://github.com/facebookresearch/audiocraft/blob/main/docs/METRICS.md.,Evaluation is documented in the paper and the code provides detailed explanation about the implementation of evaluation metrics. Evaluating the model to reproduce the results still may require some effort. Dataset used for evaluation is MusicCaps benchmark.,open,https://proceedings.neurips.cc/paper_files/paper/2023/hash/94b472a1842cd7c56dcb125fb2765fbd-Abstract-Conference.html,Accepted at NeurIPS 2023.,,open,https://github.com/facebookresearch/audiocraft/blob/main/LICENSE,The code in this repository is released under the MIT license as found in the https://github.com/facebookresearch/audiocraft/blob/main/LICENSE. The models weights in this repository are released under the CC-BY-NC 4.0 license as found in the https://github.com/facebookresearch/audiocraft/blob/main/LICENSE_weights.,star,https://github.com/facebookresearch/audiocraft/blob/main/model_cards/MUSICGEN_MODEL_CARD.md,"Model card discusses intended use, limitations and biases.",∅,,Data is copyrighted and not available.,star,https://pypi.org/project/audiocraft/,"Has two packages: audiocraft package from Meta, and transformers package from Hugging Face.",star,https://huggingface.co/spaces/facebook/MusicGen,"Demo in Hugging Face spaces, user-oriented gradio demo, and jupyter notebooks.",star,https://ai.honu.io/papers/musicgen/,Supplementary material web page with sound examples.,/projects/musicgen.yaml,91,4
https://github.com/magenta/magenta/tree/main/magenta/models/gansynth,System is covered by Apache License.,,Google Magenta,,,open,https://github.com/magenta/magenta/tree/main/magenta/models/gansynth,Source code is available.,partial,https://magenta.tensorflow.org/datasets/nsynth,"Dataset is available, but the reduced version that they use and the newly created test/train splits are not provided.",open,https://github.com/magenta/magenta/blob/main/magenta/models/gansynth/README.md,Two pretrained checkpoints are provided.,open,https://github.com/magenta/magenta/blob/main/magenta/models/gansynth/README.md,"Codebase is documented, including high-level instructions and configuration files.",open,https://arxiv.org/pdf/1902.08710,"Training procedure is fully documented, including hardware requirements and model configuration.",partial,https://arxiv.org/pdf/1902.08710,Evaluation metrics are described but exact implementations are not referenced.,open,https://openreview.net/forum?id=H1xQVn09FX,Accepted at ICLR 2019.,,open,https://github.com/magenta/magenta/blob/main/LICENSE,System is covered by Apache License.,∅,,Very few details are given of the pretrained checkpoints.,∅,,Not available.,star,https://pypi.org/project/magenta/,Available within the magenta pip package.,∅,,Not available.,star,https://storage.googleapis.com/magentadata/papers/gansynth/index.html,"Demo page is available, including sound examples of the model's capabilities.",/projects/gansynth.yaml,86,2
https://github.com/marcoppasini/musika,MIT License.,,Johannes Kepler University Linz,,,open,https://github.com/marcoppasini/musika,Codebase is available and complete.,partial,,"Training data is described in the research paper. Some of the data used is publicly available (LibriTTS corpus). However, there are some sections of the used data that are not accessible or detailed enough to fully reconstruct the dataset (South by SouthWest). Also, music coming from http://jamendo.com under “techno” genre is not detailed.",open,https://huggingface.co/musika/musika_techno/tree/main,Weights are publicly available.,open,https://github.com/marcoppasini/musika/blob/main/README.md,"Codebase is documented, including details on environment and configuration settings.",open,https://arxiv.org/pdf/2208.08706,Detailed in research paper and proper instructions are given in the model repository.,partial,https://arxiv.org/pdf/2208.08706,"FAD is used for evaluation. However, there are details missing on the evaluation procedure.",open,https://ismir2022program.ismir.net/poster_74.html,Accepted at ISMIR 2022.,,open,https://github.com/marcoppasini/musika/blob/main/LICENSE,MIT License.,∅,,Not available.,∅,,Not available.,∅,,"Not available. Source code is provided through GitHub, but not as an installable packaged or with version control.",star,https://colab.research.google.com/drive/1PowSw3doBURwLE-OTCiWkO8HVbS5paRb,"An interface is provided through Gradio. In addition, there is a colab notebook that intends to ease accessibility for non technical users.",star,https://marcoppasini.github.io/musika,Complementary page is available with sound examples and demonstration of the model's capabilities.,/projects/musika.yaml,86,2
https://github.com/RetroCirce/MusicLDM,Attribution-NonCommercial-ShareAlike 4.0 International,,"University of California San Diego, Mila-Quebec Artificial Intelligence Institute, University of Surrey, LAION",,,partial,https://github.com/RetroCirce/MusicLDM,System source code is available at GitHub repository & Hugging Face Diffusers.,partial,,"MusicLDM is trained on the Audiostock dataset, which contains 9000 music tracks for training and 1000 tracks for testing. Dataset is not directly provided, no information about the original sources accessibility or requirements is provided.",open,https://drive.google.com/drive/folders/15VDVcIgf99YRM5oGXhRxa_Rowl54uWho,Model checkpoints are available.,open,https://github.com/RetroCirce/MusicLDM/blob/main/README.md,Description on how to use the code is provided in the GitHub repo and through Hugging Face Diffusers.,open,https://arxiv.org/pdf/2308.01546,"Training procedure is documented in the research article preprint and in the appendix additional page for the peer-reviewed version (https://musicldm.github.io/appendix/). Authors include specifications on hyperparameters, GPU requirements and model configurations.",open,https://arxiv.org/pdf/2308.01546,Evaluation procedure is described in the research paper. Evaluation code is available in the model’s codebase and in its complementary repo (https://github.com/haoheliu/audioldm_eval).,open,https://ieeexplore.ieee.org/document/10447265,This article has been accepted at ICASSP 2024. The access of such paper is limited to IEEE Xplore access (https://ieeexplore.ieee.org/document/10447265). A preprint is available in arXiv (https://arxiv.org/pdf/2308.01546).,,partial,https://github.com/RetroCirce/MusicLDM/blob/main/LICENSE,Attribution-NonCommercial-ShareAlike 4.0 International,star,https://huggingface.co/ucsd-reach/musicldm,Available at HuggingFace. Information about model’s evaluation and limitations is missing and model’s architecture is just mentioned.,star,https://github.com/LAION-AI/audio-dataset/blob/main/data_card/Audiostock.md,"There is a data card. However, content is limited to data sources origin and details on how to reproduce the data collection. Details on curation and other considerations, such as consent, limitations and selection strategies are missing.",∅,,,∅,,API is shut down temporarily. Should be released again soon.,star,https://musicldm.github.io/,Demo page is available with sound examples and demonstration of the model's capabilities.,/projects/musicldm.yaml,77,3
https://github.com/hugofloresgarcia/vampnet,MIT License,,Northwestern University and Descript Inc,,,open,https://github.com/hugofloresgarcia/vampnet,System source code is available and accessible.,closed,,Dataset is not available nor is completely described.,open,https://zenodo.org/records/8136629,Model checkpoints are available. The weights of the models are licensed https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ml,open,https://github.com/hugofloresgarcia/vampnet/blob/ismir-2023/README.md,Codebase is well documented.,open,https://arxiv.org/pdf/2307.04686,"Described in the article, with multiple details on architecture, hyperparameters and GPU usage.",partial,https://arxiv.org/pdf/2307.04686,,open,https://ismir2023program.ismir.net/poster_125.html,Accepted at ISMIR 2023.,,open,https://github.com/hugofloresgarcia/vampnet/blob/ismir-2023/LICENSE,MIT License.,∅,,,∅,,,∅,,,star,https://github.com/hugofloresgarcia/vampnet/blob/ismir-2023/app.py,Gradio UI is provided in the codebase.,star,https://hugo-does-things.notion.site/VampNet-Music-Generation-via-Masked-Acoustic-Token-Modeling-e37aabd0d5f1493aa42c5711d0764b33,Complementary demo page with sound examples and demonstration of the model's capabilities is available.,/projects/vampnet.yaml,77,2
https://github.com/openai/jukebox,Codebase is licensed under a non-commercial use license by OpenAI.,,OpenAI,,,open,https://github.com/openai/jukebox,"Code is available at GitHub repository. The codebase is not actively maintained anymore, but it is still available for use. It includes the model architecture, training and evaluation code, and utilities for generating music samples.",partial,,"Training data is partially described in the preprint article. However, relevant information is missing, such as specific sources, and data itself cannot be accessed — dataset is not publicly available.",open,https://huggingface.co/openai/jukebox-5b-lyrics/tree/main,Weights are available.,open,https://github.com/openai/jukebox/blob/master/README.md,Codebase is documented.,open,https://arxiv.org/pdf/2005.00341,Information available between article and GitHub repository.,partial,https://arxiv.org/pdf/2005.00341,"Part of the evaluation procedure is documented in the research paper. However, key information is missing, such as the evaluation dataset, objective evaluation metrics and details on the manual evaluation.",partial,https://arxiv.org/abs/2005.00341,Only preprint of the research paper is available.,,partial,https://github.com/openai/jukebox/blob/master/LICENSE,Codebase is licensed under a non-commercial use license by OpenAI.,∅,,Not available.,∅,,Not available.,∅,,Not available. Codebase is officially not mantained.,∅,https://colab.research.google.com/github/openai/jukebox/blob/master/jukebox/Interacting_with_Jukebox.ipynb,Only a colab notebook is available.,star,https://openai.com/index/jukebox/,"A demo/summary page is available. However, most sonifications of the examples generations are not available anymore. There exist another site with generation examples, but most of them are not available https://jukebox.openai.com/.",/projects/jukebox.yaml,77,1
https://github.com/acids-ircam/RAVE,System covered by an CC BY-NC 4.0 license.,,IRCAM,https://www.ircam.fr/,,open,https://github.com/acids-ircam/RAVE,"Source code is available, including data preparation, model architecture, training pipeline and inference.",closed,,Training data is internal and just briefly described.,open,https://acids-ircam.github.io/rave_models_download,Several pretrained models are provided.,open,https://github.com/acids-ircam/RAVE/blob/master/README.md,"Code is thoroughly documented, allowing for understanding and reproducibility of the model.",open,https://arxiv.org/pdf/2111.05011,Training procedure is described.,open,https://arxiv.org/pdf/2111.05011,"Evaluation procedure is described, and the implementations used for the evaluation metrics are referenced.",partial,https://arxiv.org/abs/2111.05011,Preprint only. No pee-reviewed version.,,partial,https://github.com/acids-ircam/RAVE/blob/master/LICENSE,System covered by an CC BY-NC 4.0 license.,star,https://acids-ircam.github.io/rave_models_download,Model card is available.,∅,,Not available.,star,https://pypi.org/project/acids-rave/,Pip package is available.,star,https://forum.ircam.fr/projects/detail/rave-vst/,Real-time implementation of the model and tutorials are provided.,star,https://anonymous84654.github.io/RAVE_anonymous/,"Demo page is available, showcasing the model's capabilities and providing sound examples of its use.",/projects/rave.yaml,73,4
https://github.com/archinetai/audio-diffusion-pytorch,MIT license.,,"ETH Zürich, IIT Kharagpur, Max Planck Institute",,,partial,https://github.com/archinetai/audio-diffusion-pytorch,"They provide an audio diffusion library that includes different models. However, the configs shown are indicative and untested, see https://arxiv.org/abs/2301.11757 for the configs used in the paper.",partial,,"How the data is collected and acquired, including licensing issues, is detailed in the research paper. However, the exact list of songs in the dataset nor direct access to the dataset is available.",closed,,"In the GitHub repo, authors mention that “no pre-trained models are provided here”.",partial,https://github.com/archinetai/audio-diffusion-pytorch/blob/main/README.md,Code is partially describe. How to use Moûsai is not straightforward described.,open,https://arxiv.org/pdf/2301.11757,"Training procedure is describe in the article, including hardware requirements and model configuration.",partial,https://arxiv.org/pdf/2301.11757,Evaluation data or code for evaluation are not specified.,open,https://aclanthology.org/2024.acl-long.437/,Accepted at ACL 2024.,,open,https://github.com/archinetai/audio-diffusion-pytorch/blob/main/LICENSE,Code is under the MIT license.,∅,,,∅,,,star,https://pypi.org/project/audio-diffusion-pytorch/,Code belongs to https://github.com/archinetai/audio-diffusion-pytorch library.,∅,,,star,https://diligent-pansy-4cb.notion.site/Music-Generation-with-Diffusion-ebe6e9e528984fa1b226d408f6002d87,Supplementary material is available.,/projects/moûsai.yaml,55,2
https://sonycslparis.github.io/diffariff-companion/,Not available.,,Sony Computer Science Laboratories Paris and Queen Mary University of London,,,closed,,No source code.,closed,,Dataset is closed and only briefly described.,closed,,Not provided.,closed,,No source code.,open,https://arxiv.org/pdf/2406.08384,Training procedure is properly described.,partial,https://arxiv.org/pdf/2406.08384,Missing data and metrics implementations.,open,https://ismir2024program.ismir.net/poster_225.html,Accepted at ISMIR 2024.,,closed,,Not available.,∅,,Not available.,∅,,Not available.,∅,,Not available.,∅,,Not available.,star,https://sonycslparis.github.io/diffariff-companion/,A demo page with sound examples and demonstration of the models capabilities is available.,/projects/diff-a-riff.yaml,23,1
https://musiccontrolnet.github.io/web/,Not available.,,Carnegie Mellon University and Adobe Research,,,closed,,No official code repository available.,closed,,"Very limited description of the data used for training, and no sources provided.",closed,,No model weights provided.,closed,,No source code available so no documentation.,open,https://arxiv.org/pdf/2311.07069,Training procedure is described in detail in the paper.,partial,https://arxiv.org/pdf/2311.07069,"Evaluation procedure is described, but details on the specific implementation used for some of the evaluation metrics are not given, limiting the reproducibility of results.",open,https://dl.acm.org/doi/10.1109/TASLP.2024.3399026,"Accepted at IEEE/ACM Transactions on Audio, Speech, and Laguange Processing (open access).",,closed,,Not available.,∅,,No model card available.,∅,,No datasheet available.,∅,,No package available.,∅,,No user interface nor real-time implementation available.,star,https://musiccontrolnet.github.io/web/,A complementary website with sound examples and demo of the model is available.,/projects/music-controlnet.yaml,23,1
https://musiclm.com/,Not available.,,Google Research and IRCAM,,,closed,,Source code not available.,partial,,Training data is not available nor properly described.,closed,,Not provided.,closed,,Not available.,partial,https://arxiv.org/pdf/2301.11325,"Training procedure is partially documented, missing the hardware requirements.",partial,https://arxiv.org/pdf/2301.11325,"Evaluation data is provided and metrics are described, but exact implementations are not referenced.",partial,https://arxiv.org/abs/2301.11325,Peer-reviewed paper not found.,,closed,,Not available.,∅,,Not available.,∅,,Not available.,∅,,Not available.,∅,,,star,https://google-research.github.io/seanet/musiclm/examples/,Complementary page is available with sound examples and demonstration of the model's capabilities.,/projects/musiclm.yaml,23,1
Not available.,Not available.,,Google Research,,,closed,Not available.,No source codebase is available.,partial,,"Training data is described in detail. However, the resulting MuLaMCap dataset resulting from this work is not publicly available.",closed,,Model weights are not available.,closed,,No codebase is available. There is no documentation available either.,partial,https://arxiv.org/pdf/2302.03917,"The available pre-print describes partially the training procedure of the model. While some relevant information is described, such as model configuration and training details, there are key aspects of the training missing, such as hardware requirements, model checkpoints and hyperparameters.",partial,https://arxiv.org/pdf/2302.03917,"Despite the evaluation procedure is well-documented in regards to evaluation data and metrics, the evaluation process lacks some relevant details to ensure replicability. Moreover, evaluation code of the system is not available.",partial,https://arxiv.org/abs/2302.03917,Preprint is available. No peer-reviewed version of the article has been found.,,closed,,Not available.,∅,,Not available.,∅,,Not available.,∅,,Not available.,∅,,,star,https://google-research.github.io/noise2music/,Complementary site with sound examples is available.,/projects/noise2music.yaml,23,1
https://ditto-music.github.io/ditto2/,Not available.,,University of California San Diego and Adobe Research,,,closed,,No code repository.,closed,,Training data neither available nor properly described.,closed,,Not provided.,closed,,No code repository.,partial,https://arxiv.org/abs/2405.20289,Training procedure is partially described.,partial,https://arxiv.org/abs/2405.20289,Metrics and data are described but exact implementations are missing.,open,https://ismir2024program.ismir.net/poster_146.html,Accepted at ISMIR 2024,,closed,,Not available.,∅,,Not available.,∅,,Not available.,∅,,Not available.,∅,,Not available.,star,https://ditto-music.github.io/ditto2/,"Complementary demo page is available, including sound examples of the model's capabilities.",/projects/ditto-2.yaml,18,1
https://efficient-melody.github.io/,Not available.,,ByteDance,,,closed,,No source code.,closed,,Not provided and only briefly described.,closed,,Not provided.,closed,,No source code.,partial,https://arxiv.org/pdf/2305.15719,Training procedure partially described.,partial,https://arxiv.org/pdf/2305.15719,Missing information about data and specific implementations.,open,https://dl.acm.org/doi/10.5555/3666122.3666888,Accepted at NeurIPS 2023.,,closed,,Not available.,∅,,Not provided.,∅,,Not provided.,∅,,Not provided.,∅,,Not provided.,star,https://efficient-melody.github.io/#,A complementary web page with sound examples and demonstration of the model's capabilities is available.,/projects/melody.yaml,18,1
