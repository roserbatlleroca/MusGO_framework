project.link,project.background,project.license,project.notes,org.name,org.link,org.notes,sourcecode.class,sourcecode.link,sourcecode.notes,trainingdata.class,trainingdata.link,trainingdata.notes,modelweights.class,modelweights.link,modelweights.notes,codedoc.class,codedoc.link,codedoc.notes,trainprocedure.class,trainprocedure.link,trainprocedure.notes,evalprocedure.class,evalprocedure.link,evalprocedure.notes,paper.class,paper.link,paper.notes,paper.date,license.class,license.link,license.notes,modelcard.class,modelcard.link,modelcard.notes,datasheet.class,datasheet.link,datasheet.notes,package.class,package.link,package.notes,ux.class,ux.link,ux.notes,suppage.class,suppage.link,suppage.notes,source.file,openness,star_count
https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md,open,The code in this repository is released under the MIT license as found in the https://github.com/facebookresearch/audiocraft/blob/main/LICENSE. The models weights in this repository are released under the CC-BY-NC 4.0 license as found in the https://github.com/facebookresearch/audiocraft/blob/main/LICENSE_weights.,,Meta AI,,,open,https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md,Code is provided in https://github.com/facebookresearch/audiocraft/ (Training and inference),partial,,"“We rely on an internal dataset of 10K high-quality music tracks, and on the ShutterStock and Pond5 music data collections2 with respectively 25K and 365K instrument-only music tracks.” “Note that we do NOT provide any of the datasets used for training MusicGen. We provide a dummy dataset containing just a few examples for illustrative purposes.”",open,,Models are available in the GitHub repository and in Hugging Face.,open,,Training code is available and seem well documented: https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md Warning: Since data is not available is not easy to replicate.,open,,Training is well documented. https://github.com/facebookresearch/audiocraft/blob/main/docs/TRAINING.md with training pipelines and environment setup described. Note: Hardware use for training is described in the paper. ,open,,,open,https://arxiv.org/pdf/2306.05284,Accepted at NeurIPS 2023,,open,,The code in this repository is released under the MIT license as found in the https://github.com/facebookresearch/audiocraft/blob/main/LICENSE. The models weights in this repository are released under the CC-BY-NC 4.0 license as found in the https://github.com/facebookresearch/audiocraft/blob/main/LICENSE_weights.,star,,"Model card discusses intended use, limitations and biases. https://github.com/facebookresearch/audiocraft/blob/main/model_cards/MUSICGEN_MODEL_CARD.md",∅,,Data is copyrighted and not available.,star,,"Has two packages: audiocraft package from Meta, and transformers package from Hugging Face. ",star,,"Demo in Hugging Face spaces, user-oriented gradio demo, and jupyter notebooks. https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md#usage",star,,Supplementary material web page with sound examples. https://ai.honu.io/papers/musicgen/,/projects/musicgen.yaml,94,4
https://github.com/facebookresearch/audiocraft/blob/main/docs/JASCO.md,popular," Code is released under MIT, model weights are released under CC-BY-NC 4.0.",,"The Hebrew University of Jerusalem, Meta AI",,,open,https://github.com/facebookresearch/audiocraft/blob/main/docs/JASCO.md,Code is avialble at https://github.com/facebookresearch/audiocraft/blob/main/docs/JASCO.md,partial,,"They describe the datasets used to training their model. However, there is a 10k songs set that is not described (proprietary data). ",open,,Pretrained models are accessible via API ,open,,"Code is documented in GitHub repo, including installation requirements. ",partial,,"Training procedure is described in the research paper, but hardware requirements are not mentioned. ",open,,,open,https://arxiv.org/pdf/2406.10970,Accepted at ISMIR 2024. ,,open,," Code is released under MIT, model weights are released under CC-BY-NC 4.0.",star,,https://github.com/facebookresearch/audiocraft/blob/main/model_cards/JASCO_MODEL_CARD.md,∅,,,star,,Installable through audiocraft pip package,star,,API provided via Hugging Face. ,star,,https://pages.cs.huji.ac.il/adiyoss-lab/JASCO/,/projects/jasco.yaml,88,4
https://huggingface.co/stabilityai/stable-audio-open-1.0,open,Stability AI Community Licensehttps://huggingface.co/stabilityai/stable-audio-open-1.0/blob/main/LICENSE.mdNot an Open Source Initiate(OSI) approved licences ,,Stability AI,,,open,https://huggingface.co/stabilityai/stable-audio-open-1.0," Code for data processing, training pipeline, and inference is available in the stable-audio-tools repository. Architecture of the mode is specified in the form of a config file.https://github.com/Stability-AI/stable-audio-tools",open,,"Training data can be mapped from Attribution files.https://huggingface.co/stabilityai/stable-audio-open-1.0/blob/main/fma_dataset_attribution2.csv. However, accessibility is subject to accepting Stability AI conditions. ",open,,"Weights are available at: https://huggingface.co/stabilityai/stable-audio-open-1.0. However, accessibility is subject to accepting Stability AI conditions. ",open,,"Documentation of the code is limited for training replicating the model (not specific config files, but those from Stable Audio could be used). However, I think we can still consider the documentation of the code to be enough.https://huggingface.co/stabilityai/stable-audio-open-1.0https://github.com/Stability-AI/stable-audio-tools",open,,"Described in pre-print. Warning: They say they provide the exact parameters for training in the repository, but I can’t find them. ",open,,,partial,http://arxiv.org/abs/2407.14358,Paper is in arxiv but it is not peer-reviewed. ,,partial,,Stability AI Community Licensehttps://huggingface.co/stabilityai/stable-audio-open-1.0/blob/main/LICENSE.mdNot an Open Source Initiate(OSI) approved licences ,star,,Model card in HuggingFace: https://huggingface.co/stabilityai/stable-audio-open-1.0,∅,,They provide a .csv file listing each wav from Freesound and FMA. But there is no proper datasheet. It’s a pity they don’t get this start because they are careful about the data they use.    ,star,,The model can be used with: 1. the https://github.com/Stability-AI/stable-audio-tools library and 2. the https://huggingface.co/docs/diffusers/main/en/index library,∅,,I can’t find an official user-oriented application apart from the packages and code examples. But I am sure that the community around the model is building this kind of resource. We should check.,star,,They provide sound examples in https://stability.ai/news/introducing-stable-audio-open,/projects/stable audio open.yaml,88,3
https://github.com/magenta/magenta/tree/main/magenta/models/gansynth,popular,System is covered by Apache License. ,,Google Magenta,,,open,https://github.com/magenta/magenta/tree/main/magenta/models/gansynth,Source code is available.,partial,,"Dataset is available, but the reduced version that they use and the newly created test/train splits are not provided.",open,,Two pretrained checkpoints are provided.,open,,"Codebase is documented, including high-level instructions and configuration files.",open,,"Training procedure is fully documented, including hardware requirements and model configuration.",partial,,,open,https://arxiv.org/abs/1902.08710,Accepted at ICLR 2019.,,open,,System is covered by Apache License. ,∅,,Very few details are given of the pretrained checkpoints.,∅,,Not available.,star,,Available within the magenta pip package.,∅,,Not available.,star,,https://storage.googleapis.com/magentadata/papers/gansynth/index.html,/projects/gansynth.yaml,88,2
https://github.com/marcoppasini/musika,popular,MIT License. ,,Johannes Kepler University Linz,,,open,https://github.com/marcoppasini/musika,Codebase is available and complete. ,partial,,"Training data is described in the research paper. Some of the data used is publicly available: LibriTTS corpus. However, there are some sections of the used data that are not accessible or detailed enough to fully reconstruct the dataset:  South by SouthWest. Also, music coming from http://jamendo.com under “techno” genre, it not detailed. ",open,,Weights are publicly available. ,open,,"Codebase is documented, including details on environment and configuration settings. ",open,,Detailed in research paper and proper instructions are given in the model repository. ,partial,,,open,,Accepted at ISMIR 2022.,,open,,MIT License. ,∅,,Not available. ,∅,,Not available. ,∅,,"Not available. Source code is provided through GitHub, but not as an installable packaged or with version control. ",star,,"An interface is provided through Gradio. In addition, there is a colab notebook that intends to ease accessibility for non technical users: https://colab.research.google.com/drive/1PowSw3doBURwLE-OTCiWkO8HVbS5paRb",star,,https://marcoppasini.github.io/musika,/projects/musika.yaml,88,2
https://github.com/RetroCirce/MusicLDM/?tab=readme-ov-file,popular,Attribution-NonCommercial-ShareAlike 4.0 International,,"University of California San Diego, Mila-Quebec Artificial Intelligence Institute, University of Surrey, LAION",,,partial,https://github.com/RetroCirce/MusicLDM/?tab=readme-ov-file,System source code is available at GitHub repository & Hugging Face Diffusers. ,partial,,"MusicLDM is trained on the Audiostock dataset, which contains 9000 music tracks for training and 1000 tracks for testing. Dataset is not directly provided, no information about the original sources accessibility or requirements is provided. ",open,,Model checkpoints are available here: https://drive.google.com/drive/folders/15VDVcIgf99YRM5oGXhRxa_Rowl54uWho?usp=sharing,open,,Description on how to use the code is provided in the GitHub repo and through Hugging Face Diffusers. ,open,,"Training procedure is documented in the research article preprint and in the appendix additional page for the peer-reviewed version: https://musicldm.github.io/appendix/. Authors include specifications on hyperparameters, GPU requirements and model configurations. ",open,,,open,https://arxiv.org/pdf/2308.01546,This article has been accepted at ICASSP 2024. The access of such paper is limited to IEEE Xplore access: https://ieeexplore.ieee.org/document/10447265. A preprint is available in arXiv: https://arxiv.org/pdf/2308.01546,,partial,,Attribution-NonCommercial-ShareAlike 4.0 International,star,,Available at HuggingFace: https://huggingface.co/ucsd-reach/musicldm Information about model’s evaluation and limitations is missing and model’s architecture is just mentioned.  ,star,,"There is a data card: https://github.com/LAION-AI/audio-dataset/blob/main/data_card/Audiostock.md. However, content is limited to data sources origin and details on how to reproduce the data collection. Details on curation and other considerations, such as consent, limitations and selection strategies are missing. ",∅,,,∅,,API is shut down temporarily. Should be released again soon. ,star,,https://musicldm.github.io/,/projects/musicldm.yaml,81,3
https://github.com/hugofloresgarcia/vampnet,open,MIT License,,Northwestern University and Descript Inc,,,open,https://github.com/hugofloresgarcia/vampnet,System source code is available and accessible at https://github.com/hugofloresgarcia/vampnet.,closed,,Dataset is not available nor is completely described. ,open,,Model checkpoints are available https://zenodo.org/records/8136629. The weights for the models are licensed https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ml,open,,Codebase is documented. ,open,,"Described in the article, with multiple details on architecture, hyperparameters, GPU usage",partial,,,open,https://arxiv.org/abs/2307.04686,Article peer-reviewed & accepted at ISMIR 2023: https://archives.ismir.net/ismir2023/paper/000042.pdf,,open,,MIT License,∅,,,∅,,,∅,,,star,,Gradio UI is provided in the codebase. ,star,,Untitled (https://www.notion.so/e37aabd0d5f1493aa42c5711d0764b33?pvs=21) ,/projects/vampnet.yaml,81,2
https://github.com/acids-ircam/RAVE,open,System covered by an CC BY-NC 4.0 license.,,IRCAM,,,open,https://github.com/acids-ircam/RAVE,"Source code is available, including data preparation, model architecture, training pipeline and inference.",closed,,Training data is internal and just briefly described. ,open,,Several pretrained models are provided.,open,,"Code is thoroughly documented, allowing for understanding and reproducibility of the model.",open,,Training procedure is described.,open,,,partial,https://arxiv.org/pdf/2111.05011,Rejected at ICLR 2022. No peer-reviewed version. ,,partial,,System covered by an CC BY-NC 4.0 license.,star,,https://acids-ircam.github.io/rave_models_download,∅,,Not available.,star,,Pip package is available.,star,,Real-time implementation of the model and tutorials are provided.,star,,https://anonymous84654.github.io/RAVE_anonymous/,/projects/rave.yaml,75,4
https://github.com/openai/jukebox,open,Codebase is licensed under a non-commercial use license by OpenAI. ,,OpenAI,,,open,https://github.com/openai/jukebox,Code is available at https://github.com/openai/jukebox,partial,,"Training data is partially described in the preprint article. However, relevant information is missing, such as specific sources, and data itself cannot be accessed — dataset is not publicly available. ",open,,Weights are available.,open,,Codebase is documented. ,open,,Information available between article and GitHub repository.  ,partial,,,partial,https://arxiv.org/pdf/2005.00341,Only preprint of the research paper is available. ,,partial,,Codebase is licensed under a non-commercial use license by OpenAI. ,∅,,Not available. ,∅,,Not available. ,∅,,Not available. Codebase is officially not mantained. ,∅,,,star,,"A demo/summary page is available: https://openai.com/index/jukebox/However, sonifications of the examples generations are not available anymore. There exist another site with generation examples, but most of them are not available https://jukebox.openai.com/. In addition, a colab notebook is provided: https://colab.research.google.com/github/openai/jukebox/blob/master/jukebox/Interacting_with_Jukebox.ipynb",/projects/jukebox.yaml,75,1
https://github.com/archinetai/audio-diffusion-pytorch,open,Code is under the MIT license. ,,"ETH Zürich, IIT Kharagpur, Max Planck Institute",,,partial,https://github.com/archinetai/audio-diffusion-pytorch,"They provide an audio diffusion library that includes different models. However, the configs shown are indicative and untested, see https://arxiv.org/abs/2301.11757 for the configs used in the paper.",partial,,"How the data is collected and acquired, including licensing issues, is detailed in the research paper. However, the exact list of songs in the dataset nor direct access to the dataset is available. ",closed,,"In the GitHub repo, authors mention that “no pre-trained models are provided here”. ",partial,,Code is partially describe. How to use Moûsai is not straightforward described.,open,,"Training procedure is describe in the article, including hardware requirements and model configuration. ",partial,,,open,https://arxiv.org/abs/2301.11757,Accepted at ACL 2024,,open,,Code is under the MIT license. ,∅,,,∅,,,star,,Code belongs to https://github.com/archinetai/audio-diffusion-pytorch library,∅,,,star,,Supplementary material is available at Untitled (https://www.notion.so/ebe6e9e528984fa1b226d408f6002d87?pvs=21) ,/projects/moûsai.yaml,62,2
,popular,Not available.,,Sony Computer Science Laboratories Paris and Queen Mary University of London,,,closed,,No source code.,closed,,Dataset is closed and only briefly described. ,closed,,Not provided.,closed,,No source code.,open,,Training procedure is properly described.,partial,,,open,https://arxiv.org/abs/2406.08384,Accepted at ISMIR 2024,,closed,,Not available.,∅,,Not available.,∅,,Not available.,∅,,Not available.,∅,,Not available.,star,,https://sonycslparis.github.io/diffariff-companion/,/projects/diff-a-riff.yaml,31,1
,popular,Not available.,,Carnegie Mellon University and Adobe Research,,,closed,,No official code repository available.,closed,,"Very limited description of the data used for training, and no sources provided.",closed,,No model weights provided.,closed,,No source code available so no documentation.,open,,Training procedure is described in detail in the paper.,partial,,,open,https://arxiv.org/pdf/2311.07069,A peer-reviewed paper is available and publicly accessible.,,closed,,Not available.,∅,,No model card available.,∅,,No datasheet available.,∅,,No package available.,∅,,No user interface nor real-time implementation available.,star,,https://musiccontrolnet.github.io/web/,/projects/music controlnet.yaml,31,1
,popular,Not available.,,University of California San Diego and Adobe Research,,,closed,,No code repository.,closed,,Training data neither available nor properly described.,closed,,Not provided.,closed,,No code repository.,partial,,Training procedure is partially described.,partial,,,open,https://arxiv.org/abs/2405.20289,Accepted at ISMIR 2024,,closed,,Not available.,∅,,Not available.,∅,,Not available.,∅,,Not available.,∅,,Not available.,star,,https://ditto-music.github.io/ditto2/,/projects/ditto-2.yaml,25,1
,popular,Not available.,,ByteDance,,,closed,,No source code.,closed,,Not provided and only briefly described.,closed,,Not provided.,closed,,No source code.,partial,,Training procedure partially described.,partial,,,open,https://arxiv.org/pdf/2305.15719,Accepted at NeurIPS 2023: https://dl.acm.org/doi/10.5555/3666122.3666888,,closed,,Not available.,∅,,Not provided.,∅,,Not provided.,∅,,Not provided.,∅,,Not provided.,star,,https://efficient-melody.github.io/#,/projects/melody.yaml,25,1
,popular,Not available.,,Google Research and IRCAM,,,closed,,Source code not available.,partial,,Training data is not available nor properly described.,closed,,Not provided.,closed,,Not available.,partial,,"Training procedure is partially documented, missing the hardware requirements.",partial,,,partial,https://arxiv.org/pdf/2301.11325,Peer-reviewed paper not found.,,closed,,Not available.,∅,,Not available.,∅,,Not available.,∅,,Not available.,∅,,In theory available to try in AI Test Kitchen but I never managed?,star,,https://google-research.github.io/seanet/musiclm/examples/,/projects/musiclm.yaml,25,1
Not available. ,popular,Not available. ,,Google Research,,,closed,Not available. ,No source codebase is available. ,partial,,"Training data is described in detail. However, the resulting MuLaMCap dataset resulting from this work is not publicly available.",closed,,Model weights are not available. ,closed,,No codebase is available. There is no documentation available either. ,partial,,"The available preprint describes partially the training procedure of the model. While some relevant information is described, such as model configuration and training details, there are key aspects of the training missing, such as hardware requirements, model checkpoints and hyperparameters. ",partial,,,partial,https://arxiv.org/pdf/2302.03917,Preprint is available. No peer-reviewed version of the article has been found. ,,closed,,Not available. ,∅,,Not available. ,∅,,Not available. ,∅,,Not available. ,∅,,,star,,https://google-research.github.io/noise2music/,/projects/noise2music.yaml,25,1
