<!DOCTYPE html>

<html>
<head>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>MusGO Framework: Assessing Openness in Music-Generative AI</title>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&amp;display=swap" rel="stylesheet"/>
<link href="styles.css" rel="stylesheet">
<script data-domain="roserbatlleroca.github.io/MusGO_framework/" defer="" src="https://plausible.io/js/script.js"></script>
<style>
  body {
  font-family: 'Open Sans', sans-serif;
  }
  .highlight {
    background-color: #f0f0f0;
    padding: 1em;
    /* border-left: 4px solid #ccc; */
  }
  #table-guide em {
    font-size: 1em;
    font-weight: bold;
    margin-bottom: 0.5em;
  }
  #table-guide {
  background-color: #f8f8f8;
  padding: 1em;
  font-size: 1em;

  }
  #table-guide p {
  margin-top: 0.5em;
  margin-bottom: 0.5em;
  }
  #table-guide {
  background-color: #f8f8f8;
  padding: 0.5em 1em 0.5em 1em; /* top, right, bottom, left */
  font-size: 1em;
}
#footer {
  padding: 1em;
  background-color: #f0f0f0;
  border-top: 1px solid #ccc;
}

.footer-content {
  display: flex;
  justify-content: space-between;
  align-items: center;
  flex-wrap: wrap;
}

.footer-text {
  max-width: 70%;
}

.footer-image img {
  max-height: 60px; /* adjust as needed */
  width: auto;
}


</style>
</link></head>
<body>
<div id="header">
<h1>MusGO Framework: Assessing Openness in Music-Generative AI</h1>
</div>
<div id="content">
<p class="highlight" id="description">
    The <strong>Music-Generative Open AI (MusGO)</strong> framework is a community-driven framework designed to assess the openness of music-generative models. 
    With a collaborative approach, it invites contributions from researchers and artists, supports public scrutiny, and enables tracking of model evolution to promote transparency, accountability, and responsible development.
  </p>
<p>
This website builds on the paper <em>“MusGO: A Community-Driven Framework for Assessing Openness in Music-Generative AI”</em>, authored by Roser Batlle-Roca, Laura Ibáñez-Martínez, Xavier Serra, 
Emilia Gómez, and Martín Rocamora.  
It serves not only as a companion to the publication, but also as a living resource, which is continuously updated and shaped by contributions from the community. 
<br/>
<br/><a href="http://arxiv.org/pdf/2507.03599">Read the paper</a>  
  | <a href="https://github.com/roserbatlleroca/MusGO_framework">GitHub Repository</a>
  | <a href="framework.html">Detailed Criteria</a>
  | <a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/README.md">How to contribute?</a>
  | <a href="survey.html">MIR Survey Results</a>
  | <a href="https://github.com/roserbatlleroca/MusGO_framework/tree/main?tab=readme-ov-file#-any-doubts-or-thoughts-help-us-improve">Help us improve!</a>
</p>
<h2>Openness Leaderdboard</h2>
<div class="highlight" id="table-guide"><p>
<em>How to interpret this table?</em> MusGO framework consists of 13 dimensions of openness, distinguishing between <em>essential</em> (1–8) and <em>desirable</em> (9–13) categories.  
    Essential categories follow a three-level scale:  
    (<span class="openness open"><strong>✔︎</strong> open</span>,  
    <span class="openness partial"><strong>~</strong> partial</span>, or  
    <span class="openness closed"><strong>✘</strong> closed</span>).  
    Desirable categories are binary, indicating whether an element exists (⭐) or not.  

  </p>
<p>
    Models are ordered using a weighted
    openness score (O), based on essential categories (E) and
    normalised to a 100-point scale. Considering survey findings,
    the three most relevant categories (E<sub>1</sub>, E<sub>2</sub> and E<sub>3</sub>,
    all with M<sub>1,2,3</sub>=5) are weighted twice as much as the others.
    Note that the score is used for ordering purposes only,
    and we do not intend to reduce openness to a single value.
    When models achieve the same score, the order is determined
    by the highest number of fulfilled desirable categories.
  </p>
<p>
    Each cell includes <strong>interactive elements</strong>: 
    <ul>
<li>Hovering over a cell reveals a tooltip with the justification behind the assigned score.</li>
<li>Clicking on a cell redirects you to the source of information or relevant supplementary material (e.g., research paper, source code, model checkpoints, etc.).</li>
</ul>

    For a detailed brakedown of each model’s evaluation, you can explore its corresponding YAML file in the <a href="https://github.com/roserbatlleroca/MusGO_framework/tree/main/projects">project folder</a>.
  </p>
</div>
<br/>
<div id="included-table"><table>
<thead>
<tr class="main-header"><th>Project</th><th colspan="8">Essential</th><th colspan="5">Desirable</th>
<tr class="second-header"><th></th><th>Source code</th><th>Training data</th><th>Model weights</th><th>Code<br/>documentation</th><th>Training<br/>procedure</th><th>Evaluation<br/>procedure</th><th>Research paper</th><th>License</th><th>Model card</th><th>Datasheet</th><th>Package</th><th>UX<br/>application</th><th>Supplementary<br/>material page</th>
</tr></tr></thead>
<tbody>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/stable-audio-open.yaml" target="_blank" title="data: stable-audio-open.yaml">Stable Audio Open</a></td><td class="open data-cell"><a href="https://github.com/Stability-AI/stable-audio-tools" target="_blank" title="Code for data processing, training pipeline, and inference is available in the stable-audio-tools repository. Architecture of the mode is specified in the form of a config file.">✔︎</a></td><td class="open data-cell"><a href="https://info.stability.ai/attributions" target="_blank" title="Training data can be mapped from Attribution files. Dataset attribution files can also be accessed in HuggingFace after accepting Stability AI conditions (https://huggingface.co/stabilityai/stable-audio-open-1.0/blob/main/fma_dataset_attribution2.csv and https://huggingface.co/stabilityai/stable-audio-open-1.0/blob/main/freesound_dataset_attribution2.csv).">✔︎</a></td><td class="open data-cell"><a href="https://huggingface.co/stabilityai/stable-audio-open-1.0/tree/main" target="_blank" title="Weights are available in the HuggingFace repository. However, accessibility is subject to accepting Stability AI conditions.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/Stability-AI/stable-audio-tools/blob/main/README.md" target="_blank" title="Documentation of the code is limited for training replicating the model. Although there are not specific config files for Stable Audio Open, those from Stable Audio could be used.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2407.14358" target="_blank" title="Described in pre-print.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2407.14358" target="_blank" title="They use stable-audio-metrics for evaluation, which has examples of evaluation pipelines.">✔︎</a></td><td class="open data-cell"><a href="https://ieeexplore.ieee.org/document/10888461" target="_blank" title="Accepted at ICASSP 2026.">✔︎</a></td><td class="partial data-cell"><a href="https://huggingface.co/stabilityai/stable-audio-open-1.0/blob/main/LICENSE.md" target="_blank" title="Stability AI Community License. Not an Open Source Initiate (OSI) or RAIL approved licences.">~</a></td><td class="star data-cell"><a href="https://huggingface.co/stabilityai/stable-audio-open-1.0" target="_blank" title="Model card in HuggingFace.">⭐</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="star data-cell"><a href="https://pypi.org/project/stable-audio-tools/" target="_blank" title="The model can be used with 1) the https://github.com/Stability-AI/stable-audio-tools library and 2) the https://huggingface.co/docs/diffusers/main/en/index library.">⭐</a></td><td class="star data-cell"><a href="https://github.com/Stability-AI/stable-audio-tools/blob/main/stable_audio_tools/interface/gradio.py" target="_blank" title="Gradio interface in source code.">⭐</a></td><td class="star data-cell"><a href="https://stability.ai/news/introducing-stable-audio-open" target="_blank" title="A complementary site with sound examples and demonstration of the model's capabilities is available.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/Stability-AI/stable-audio-tools" target="_blank" title=""><span style="display:inline-block;min-width:36px">2024</span>Stability AI</a></td><td colspan="7"></td><td class="source-link"><a href="https://stability.ai/" target="_blank" title="Stability AI"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/musicgen.yaml" target="_blank" title="data: musicgen.yaml">MusicGen</a></td><td class="open data-cell"><a href="https://github.com/facebookresearch/audiocraft" target="_blank" title="Code is available.">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="They describe the datasets used to training their model (licensed data). However, they rely on 10k music tracks that are not described (proprietary data). They do not provide any of the datasets used for training MusicGen. Only a dummy dataset containing just a few examples for illustrative purposes is provided.">~</a></td><td class="open data-cell"><a href="https://huggingface.co/facebook/musicgen-large/tree/main" target="_blank" title="Models are available in the GitHub repository and in Hugging Face.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md" target="_blank" title="Codebase is documented in the GitHub repository, including installation requirements.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2306.05284" target="_blank" title="Training procedure is well documented in the paper, including hardware requirements and model configurations. Additional implementation details are available in the GitHub repository.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2306.05284" target="_blank" title="Evaluation is documented in the paper and the code provides detailed explanation about the implementation of evaluation metrics. Evaluating the model to reproduce the results still may require some effort. Dataset used for evaluation is MusicCaps benchmark.">✔︎</a></td><td class="open data-cell"><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/94b472a1842cd7c56dcb125fb2765fbd-Abstract-Conference.html" target="_blank" title="Accepted at NeurIPS 2023.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/facebookresearch/audiocraft/blob/main/LICENSE" target="_blank" title="The code in this repository is released under the MIT license as found in the https://github.com/facebookresearch/audiocraft/blob/main/LICENSE. The models weights in this repository are released under the CC-BY-NC 4.0 license as found in the https://github.com/facebookresearch/audiocraft/blob/main/LICENSE_weights.">✔︎</a></td><td class="star data-cell"><a href="https://github.com/facebookresearch/audiocraft/blob/main/model_cards/MUSICGEN_MODEL_CARD.md" target="_blank" title="Model card discusses intended use, limitations and biases.">⭐</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Data is copyrighted and not available.">∅</a></td><td class="star data-cell"><a href="https://pypi.org/project/audiocraft/" target="_blank" title="Has two packages: audiocraft package from Meta, and transformers package from Hugging Face.">⭐</a></td><td class="star data-cell"><a href="https://huggingface.co/spaces/facebook/MusicGen" target="_blank" title="Demo in Hugging Face spaces, user-oriented gradio demo, and jupyter notebooks.">⭐</a></td><td class="star data-cell"><a href="https://ai.honu.io/papers/musicgen/" target="_blank" title="Supplementary material web page with sound examples.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/facebookresearch/audiocraft" target="_blank" title=""><span style="display:inline-block;min-width:36px">2023</span>Meta AI</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Meta AI"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/song-gen.yaml" target="_blank" title="data: song-gen.yaml">SongGen</a></td><td class="open data-cell"><a href="https://github.com/LiuZH-19/SongGen" target="_blank" title="The source code is available on GitHub, but there are some parts missing regarding data processing (indicated as to-do in GitHub).">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="This model is trained on 8,000 hours of openly licensed music data, including MSD, FMA and MTG-Jamendo. However, the curated version of the dataset is not available (indicated as to-do in GitHub).">~</a></td><td class="open data-cell"><a href="https://huggingface.co/LiuZH-19/SongGen_mixed_pro/tree/main" target="_blank" title="Weights are available on Hugging Face.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/LiuZH-19/SongGen/blob/master/README.md" target="_blank" title="Codebase is well documented, with a README file that includes installation and inference instructions, and links to the training procedure.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2502.13128" target="_blank" title="The training procedure is described in the article (see Appendix A). Also, there are specific training guidelines in the GitHub repository.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2502.13128" target="_blank" title="Evaluation procedure is described in the article (see Appendix B). The dataset used for evaluation is available on Hugging Face.">✔︎</a></td><td class="open data-cell"><a href="https://openreview.net/pdf?id=aTyQjpsaB4" target="_blank" title="Accepted at ICML 2025.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/LiuZH-19/SongGen/tree/master?tab=Apache-2.0-1-ov-file" target="_blank" title="Apache License 2.0">✔︎</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="star data-cell"><a href="https://liuzh-19.github.io/SongGen/" target="_blank" title="A demo page with sound examples is available. The page also includes a link to the GitHub repository and research paper.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/LiuZH-19/SongGen" target="_blank" title=""><span style="display:inline-block;min-width:36px">2025</span>Beihang University, Shanghai AI Laboratory, The Chinese University of Hong Kong, Harbin Institute of Technology, CPII (InnoHK)</a></td><td colspan="7"></td><td class="source-link"><a href="https://ev.buaa.edu.cn/" target="_blank" title="Beihang University, Shanghai AI Laboratory, The Chinese University of Hong Kong, Harbin Institute of Technology, CPII (InnoHK)"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/gansynth.yaml" target="_blank" title="data: gansynth.yaml">GANsynth</a></td><td class="open data-cell"><a href="https://github.com/magenta/magenta/tree/main/magenta/models/gansynth" target="_blank" title="Source code is available.">✔︎</a></td><td class="partial data-cell"><a href="https://magenta.tensorflow.org/datasets/nsynth" target="_blank" title="Dataset is available, but the reduced version that they use and the newly created test/train splits are not provided.">~</a></td><td class="open data-cell"><a href="https://github.com/magenta/magenta/blob/main/magenta/models/gansynth/README.md" target="_blank" title="Two pretrained checkpoints are provided.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/magenta/magenta/blob/main/magenta/models/gansynth/README.md" target="_blank" title="Codebase is documented, including high-level instructions and configuration files.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/1902.08710" target="_blank" title="Training procedure is fully documented, including hardware requirements and model configuration.">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/1902.08710" target="_blank" title="Evaluation metrics are described but exact implementations are not referenced.">~</a></td><td class="open data-cell"><a href="https://openreview.net/forum?id=H1xQVn09FX" target="_blank" title="Accepted at ICLR 2019.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/magenta/magenta/blob/main/LICENSE" target="_blank" title="System is covered by Apache License.">✔︎</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Very few details are given of the pretrained checkpoints.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="star data-cell"><a href="https://pypi.org/project/magenta/" target="_blank" title="Available within the magenta pip package.">⭐</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="star data-cell"><a href="https://storage.googleapis.com/magentadata/papers/gansynth/index.html" target="_blank" title="Demo page is available, including sound examples of the model's capabilities.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/magenta/magenta/tree/main/magenta/models/gansynth" target="_blank" title=""><span style="display:inline-block;min-width:36px">2019</span>Google Magenta</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Google Magenta"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/musika.yaml" target="_blank" title="data: musika.yaml">Musika</a></td><td class="open data-cell"><a href="https://github.com/marcoppasini/musika" target="_blank" title="Codebase is available and complete.">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="Training data is described in the research paper. Some of the data used is publicly available (LibriTTS corpus). However, there are some sections of the used data that are not accessible or detailed enough to fully reconstruct the dataset (South by SouthWest). Also, music coming from http://jamendo.com under “techno” genre is not detailed.">~</a></td><td class="open data-cell"><a href="https://huggingface.co/musika/musika_techno/tree/main" target="_blank" title="Weights are publicly available.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/marcoppasini/musika/blob/main/README.md" target="_blank" title="Codebase is documented, including details on environment and configuration settings.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2208.08706" target="_blank" title="Detailed in research paper and proper instructions are given in the model repository.">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2208.08706" target="_blank" title="FAD is used for evaluation. However, there are details missing on the evaluation procedure.">~</a></td><td class="open data-cell"><a href="https://ismir2022program.ismir.net/poster_74.html" target="_blank" title="Accepted at ISMIR 2022.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/marcoppasini/musika/blob/main/LICENSE" target="_blank" title="MIT License.">✔︎</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available. Source code is provided through GitHub, but not as an installable packaged or with version control.">∅</a></td><td class="star data-cell"><a href="https://colab.research.google.com/drive/1PowSw3doBURwLE-OTCiWkO8HVbS5paRb" target="_blank" title="An interface is provided through Gradio. In addition, there is a colab notebook that intends to ease accessibility for non technical users.">⭐</a></td><td class="star data-cell"><a href="https://marcoppasini.github.io/musika" target="_blank" title="Complementary page is available with sound examples and demonstration of the model's capabilities.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/marcoppasini/musika" target="_blank" title=""><span style="display:inline-block;min-width:36px">2022</span>Johannes Kepler University Linz</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Johannes Kepler University Linz"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/musicldm.yaml" target="_blank" title="data: musicldm.yaml">MusicLDM</a></td><td class="partial data-cell"><a href="https://github.com/RetroCirce/MusicLDM" target="_blank" title="System source code is available at GitHub repository &amp; Hugging Face Diffusers.">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="MusicLDM is trained on the Audiostock dataset, which contains 9000 music tracks for training and 1000 tracks for testing. Dataset is not directly provided, no information about the original sources accessibility or requirements is provided.">~</a></td><td class="open data-cell"><a href="https://drive.google.com/drive/folders/15VDVcIgf99YRM5oGXhRxa_Rowl54uWho" target="_blank" title="Model checkpoints are available.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/RetroCirce/MusicLDM/blob/main/README.md" target="_blank" title="Description on how to use the code is provided in the GitHub repo and through Hugging Face Diffusers.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2308.01546" target="_blank" title="Training procedure is documented in the research article preprint and in the appendix additional page for the peer-reviewed version (https://musicldm.github.io/appendix/). Authors include specifications on hyperparameters, GPU requirements and model configurations.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2308.01546" target="_blank" title="Evaluation procedure is described in the research paper. Evaluation code is available in the model’s codebase and in its complementary repo (https://github.com/haoheliu/audioldm_eval).">✔︎</a></td><td class="open data-cell"><a href="https://ieeexplore.ieee.org/document/10447265" target="_blank" title="This article has been accepted at ICASSP 2024. The access of such paper is limited to IEEE Xplore access (https://ieeexplore.ieee.org/document/10447265). A preprint is available in arXiv (https://arxiv.org/pdf/2308.01546).">✔︎</a></td><td class="partial data-cell"><a href="https://github.com/RetroCirce/MusicLDM/blob/main/LICENSE" target="_blank" title="Attribution-NonCommercial-ShareAlike 4.0 International">~</a></td><td class="star data-cell"><a href="https://huggingface.co/ucsd-reach/musicldm" target="_blank" title="Available at HuggingFace. Information about model’s evaluation and limitations is missing and model’s architecture is just mentioned.">⭐</a></td><td class="star data-cell"><a href="https://github.com/LAION-AI/audio-dataset/blob/main/data_card/Audiostock.md" target="_blank" title="There is a data card. However, content is limited to data sources origin and details on how to reproduce the data collection. Details on curation and other considerations, such as consent, limitations and selection strategies are missing.">⭐</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="API is shut down temporarily. Should be released again soon.">∅</a></td><td class="star data-cell"><a href="https://musicldm.github.io/" target="_blank" title="Demo page is available with sound examples and demonstration of the model's capabilities.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/RetroCirce/MusicLDM" target="_blank" title=""><span style="display:inline-block;min-width:36px">2023</span>University of California San Diego, Mila-Quebec Artificial Intelligence Institute, University of Surrey, LAION</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="University of California San Diego, Mila-Quebec Artificial Intelligence Institute, University of Surrey, LAION"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/vampnet.yaml" target="_blank" title="data: vampnet.yaml">VampNet</a></td><td class="open data-cell"><a href="https://github.com/hugofloresgarcia/vampnet" target="_blank" title="System source code is available and accessible.">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Dataset is not available nor is completely described.">✘</a></td><td class="open data-cell"><a href="https://zenodo.org/records/8136629" target="_blank" title="Model checkpoints are available. The weights of the models are licensed https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ml">✔︎</a></td><td class="open data-cell"><a href="https://github.com/hugofloresgarcia/vampnet/blob/ismir-2023/README.md" target="_blank" title="Codebase is well documented.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2307.04686" target="_blank" title="Described in the article, with multiple details on architecture, hyperparameters and GPU usage.">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2307.04686" target="_blank" title="">~</a></td><td class="open data-cell"><a href="https://ismir2023program.ismir.net/poster_125.html" target="_blank" title="Accepted at ISMIR 2023.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/hugofloresgarcia/vampnet/blob/ismir-2023/LICENSE" target="_blank" title="MIT License.">✔︎</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="star data-cell"><a href="https://github.com/hugofloresgarcia/vampnet/blob/ismir-2023/app.py" target="_blank" title="Gradio UI is provided in the codebase.">⭐</a></td><td class="star data-cell"><a href="https://hugo-does-things.notion.site/VampNet-Music-Generation-via-Masked-Acoustic-Token-Modeling-e37aabd0d5f1493aa42c5711d0764b33" target="_blank" title="Complementary demo page with sound examples and demonstration of the model's capabilities is available.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/hugofloresgarcia/vampnet" target="_blank" title=""><span style="display:inline-block;min-width:36px">2023</span>Northwestern University and Descript Inc</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Northwestern University and Descript Inc"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/jukebox.yaml" target="_blank" title="data: jukebox.yaml">Jukebox</a></td><td class="open data-cell"><a href="https://github.com/openai/jukebox" target="_blank" title="Code is available at GitHub repository. The codebase is not actively maintained anymore, but it is still available for use. It includes the model architecture, training and evaluation code, and utilities for generating music samples.">✔︎</a></td><td class="partial data-cell"><a href="" target="_blank" title="Training data is partially described in the preprint article. However, relevant information is missing, such as specific sources, and data itself cannot be accessed — dataset is not publicly available.">~</a></td><td class="open data-cell"><a href="https://huggingface.co/openai/jukebox-5b-lyrics/tree/main" target="_blank" title="Weights are available.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/openai/jukebox/blob/master/README.md" target="_blank" title="Codebase is documented.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2005.00341" target="_blank" title="Information available between article and GitHub repository.">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2005.00341" target="_blank" title="Part of the evaluation procedure is documented in the research paper. However, key information is missing, such as the evaluation dataset, objective evaluation metrics and details on the manual evaluation.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2005.00341" target="_blank" title="Only preprint of the research paper is available.">~</a></td><td class="partial data-cell"><a href="https://github.com/openai/jukebox/blob/master/LICENSE" target="_blank" title="Codebase is licensed under a non-commercial use license by OpenAI.">~</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available. Codebase is officially not mantained.">∅</a></td><td class="∅ data-cell"><a href="https://colab.research.google.com/github/openai/jukebox/blob/master/jukebox/Interacting_with_Jukebox.ipynb" target="_blank" title="Only a colab notebook is available.">∅</a></td><td class="star data-cell"><a href="https://openai.com/index/jukebox/" target="_blank" title="A demo/summary page is available. However, most sonifications of the examples generations are not available anymore. There exist another site with generation examples, but most of them are not available https://jukebox.openai.com/.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/openai/jukebox" target="_blank" title=""><span style="display:inline-block;min-width:36px">2020</span>OpenAI</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="OpenAI"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/rave.yaml" target="_blank" title="data: rave.yaml">RAVE</a></td><td class="open data-cell"><a href="https://github.com/acids-ircam/RAVE" target="_blank" title="Source code is available, including data preparation, model architecture, training pipeline and inference.">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Training data is internal and just briefly described.">✘</a></td><td class="open data-cell"><a href="https://acids-ircam.github.io/rave_models_download" target="_blank" title="Several pretrained models are provided.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/acids-ircam/RAVE/blob/master/README.md" target="_blank" title="Code is thoroughly documented, allowing for understanding and reproducibility of the model.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2111.05011" target="_blank" title="Training procedure is described.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2111.05011" target="_blank" title="Evaluation procedure is described, and the implementations used for the evaluation metrics are referenced.">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2111.05011" target="_blank" title="Preprint only. No pee-reviewed version.">~</a></td><td class="partial data-cell"><a href="https://github.com/acids-ircam/RAVE/blob/master/LICENSE" target="_blank" title="System covered by an CC BY-NC 4.0 license.">~</a></td><td class="star data-cell"><a href="https://acids-ircam.github.io/rave_models_download" target="_blank" title="Model card is available.">⭐</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="star data-cell"><a href="https://pypi.org/project/acids-rave/" target="_blank" title="Pip package is available.">⭐</a></td><td class="star data-cell"><a href="https://forum.ircam.fr/projects/detail/rave-vst/" target="_blank" title="Real-time implementation of the model and tutorials are provided.">⭐</a></td><td class="star data-cell"><a href="https://anonymous84654.github.io/RAVE_anonymous/" target="_blank" title="Demo page is available, showcasing the model's capabilities and providing sound examples of its use.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/acids-ircam/RAVE" target="_blank" title=""><span style="display:inline-block;min-width:36px">2021</span>IRCAM</a></td><td colspan="7"></td><td class="source-link"><a href="https://www.ircam.fr/" target="_blank" title="IRCAM"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/jam.yaml" target="_blank" title="data: jam.yaml">JAM</a></td><td class="open data-cell"><a href="https://github.com/declare-lab/jamify" target="_blank" title="Source code is available in the GitHub repository, including data processing, training and inference code, as well as model architecture.">✔︎</a></td><td class="closed data-cell"><a 4.1.="" dataset="" href="https://arxiv.org/pdf/2507.20880" in="" of="" pre-print."="" section="" setup"="" target="_blank" the="" title="Training data is not available nor properly described. In the research paper, they only indicate that they rely on a ~1 million song dataset, resulting into ~54k hours of audion. No information on specific data sources, acquisition methods, and data licensing is given. See ">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/declare-lab/JAM-0.5/blob/main/jam-0_5.safetensors" target="_blank" title="Pre-trained version of the model is available in Hugging Face.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/declare-lab/jamify/blob/main/README.md" target="_blank" title="Codebase is documented, including high-level instructions and configuration files.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2507.20880" target="_blank" title="Training procedure is properly described, including hardware requirements and confirgurations (see Section 4.2).">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2507.20880" target="_blank" title="The evaluation procedure is properly described, including evaluation data, metrics, and implementation details (see Section 4.3). With the aim to standarize evaluation across models, they propose JAME: a genre-driverse evaluation dataset for full-track generation. This dataset is publicly available in Hugging Face (https://huggingface.co/datasets/declare-lab/JAME).">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2507.20880" target="_blank" title="Pre-print is available. No peer-reviewed version of the article has been found.">~</a></td><td class="partial data-cell"><a href="https://github.com/declare-lab/jamify/blob/main/STABILITY_AI_COMMUNITY_LICENSE.md" target="_blank" title="JAM is released under the Project Jamify License and the Stability AI Community License Agreement, which allow for non-commercial use and research purposes with certain restrictions.">~</a></td><td class="star data-cell"><a href="https://huggingface.co/declare-lab/JAM-0.5" target="_blank" title="Model card available in Hugging Face.">⭐</a></td><td class="∅ data-cell"><a href="https://huggingface.co/datasets/declare-lab/JAME" target="_blank" title="Dataset card for the evaluation dataset (JAME) is available in Hugging Face. However, no datasheet is provided for the training data used to train JAM.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="star data-cell"><a href="https://huggingface.co/spaces/declare-lab/JAM" target="_blank" title="There is a user-oriented application on Hugging Face Spaces, but it is currently not available. Nonetheless, all the files to run the app are provided in the HuggingFace repository. See https://huggingface.co/spaces/declare-lab/JAM/tree/main, and https://huggingface.co/spaces/declare-lab/JAM/blob/main/app.py.">⭐</a></td><td class="star data-cell"><a href="https://declare-lab.github.io/jamify" target="_blank" title="A supplementary material page is available showcasing the use of JAM.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://declare-lab.github.io/jamify" target="_blank" title="A tiny flow-based song generator with fine-grained controllability and aesthetic alignment"><span style="display:inline-block;min-width:36px">2025</span>Singapore University of Technology and Design and Lamda Labs</a></td><td colspan="7"></td><td class="source-link"><a href="https://www.sutd.edu.sg/" target="_blank" title="Singapore University of Technology and Design and Lamda Labs"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/after.yaml" target="_blank" title="data: after.yaml">AFTER</a></td><td class="open data-cell"><a href="https://github.com/acids-ircam/AFTER" target="_blank" title="Source code is available in the GitHub repository, including data processing, training and inference pipelines, as well as model architecture.">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2408.00196" target="_blank" title="Datasets used for training the original model (SLAKH, MaestroV2, GuitarSet and URMP) are publicly available and described in the paper, but the datasets used for training the current model are unknown.">~</a></td><td class="open data-cell"><a href="https://nubo.ircam.fr/index.php/s/8NFD5gWwbkT4G5P" target="_blank" title="Weights are available to download from the link provided in the GitHub repository.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/acids-ircam/AFTER/blob/main/README.md" target="_blank" title="Documentation is available in the GitHub repository, including installation requirements, examples of scripts usage and configuration files. The codebase could still benefit from more detailed documentation.">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2408.00196" target="_blank" title="Training procedure of the original model is fully described in the paper, but relevant changes in the model architecture and training procedure are not properly documented.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2408.00196" target="_blank" title="Evaluation procedure of the original model is fully described in the paper, including used metrics and implementation details. However, no evaluation is provided for the current model.">~</a></td><td class="partial data-cell"><a href="https://zenodo.org/records/14877437" target="_blank" title="Original paper is publicly available and peer-reviewed, but no publication is available for the current model.">~</a></td><td class="partial data-cell"><a href="https://github.com/acids-ircam/AFTER/blob/main/LICENSE.md" target="_blank" title="Creative Commons Attribution-NonCommercial 4.0 International license.">~</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="star data-cell"><a href="https://nubo.ircam.fr/index.php/s/8NFD5gWwbkT4G5P" target="_blank" title="Max/MSP patches with real-time implementation of the model are provided.">⭐</a></td><td class="star data-cell"><a href="https://nilsdem.github.io/control-transfer-diffusion/" target="_blank" title="There is a supplementary page for the original model.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/acids-ircam/AFTER" target="_blank" title="AFTER is the real-time implementation of the model introduced in 'Combining audio control and style transfer using latent diffusion' by Dermerlé et al. (2024), which we refer to as the original model in this evaluation."><span style="display:inline-block;min-width:36px">2024</span>IRCAM</a></td><td colspan="7"></td><td class="source-link"><a href="https://www.ircam.fr/" target="_blank" title="IRCAM"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/yue.yaml" target="_blank" title="data: yue.yaml">YuE (乐)</a></td><td class="partial data-cell"><a href="https://github.com/multimodal-art-projection/YuE" target="_blank" title="Source code is available in the GitHub repo, including inference code and model architecture. They also provide instructions on how to fine-tune YuE for specific data (see https://github.com/multimodal-art-projection/YuE/tree/main/finetune), including data processing and training procedure. Code to train the model from scratch is missing.">~</a></td><td class="closed data-cell"><a 5.1.="" data="" href="https://arxiv.org/pdf/2503.08638" in="" of="" report."="" section="" setup"="" target="_blank" technical="" the="" title="Speech-related datasets are introduced (i.e., WeNetSpeech (zh), LibriHeavy (en), and GigaSpeech (en)). But for music, they only indicate that they rely on 650K hours of music recordings. No information on specific data sources, acquisition methods, and data licensing is given. See ">✘</a></td><td class="open data-cell"><a href="https://huggingface.co/collections/m-a-p/yue" target="_blank" title="Checkpoints are available in Hugging Face.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/multimodal-art-projection/YuE" target="_blank" title="Codebase is documented in the GitHub repository, including installation requirements and usage instructions.">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2503.08638" target="_blank" title="Training procedure is properly described, including hardware requirements and confirgurations (see Section 4 and Section 5.1).">✔︎</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2503.08638" target="_blank" title="Evaluation procedure is described in Section 5.2 of the technical report, including both objective and subjective evaluation methods. Evaluation metrics implementations are referenced in the text.">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2503.08638" target="_blank" title="Technical report available, but no peer-reviewed publication yet.">~</a></td><td class="open data-cell"><a href="https://github.com/multimodal-art-projection/YuE?tab=Apache-2.0-1-ov-file" target="_blank" title="Apache License 2.0">✔︎</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="No official user-oriented application is available. But there are user-oriented applications available for YuE, for both Windows and Linux. For Windows, check out: https://github.com/multimodal-art-projection/YuE?tab=readme-ov-file#-windows-users-quickstart, and for Linux: https://github.com/multimodal-art-projection/YuE?tab=readme-ov-file#-linuxwsl-users-quickstart.">∅</a></td><td class="star data-cell"><a href="https://map-yue.github.io/" target="_blank" title="A demo page is available showcasing the use of YuE.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/multimodal-art-projection/YuE" target="_blank" title=""><span style="display:inline-block;min-width:36px">2025</span>The Hong Kong University of Science and Technology and Multimodela Art Projection (M-A-P)</a></td><td colspan="7"></td><td class="source-link"><a href="https://hkust.edu.hk/" target="_blank" title="The Hong Kong University of Science and Technology and Multimodela Art Projection (M-A-P)"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/moûsai.yaml" target="_blank" title="data: moûsai.yaml">Moûsai</a></td><td class="partial data-cell"><a href="https://github.com/archinetai/audio-diffusion-pytorch" target="_blank" title="They provide an audio diffusion library that includes different models. However, the configs shown are indicative and untested, see https://arxiv.org/abs/2301.11757 for the configs used in the paper.">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="How the data is collected and acquired, including licensing issues, is detailed in the research paper. However, the exact list of songs in the dataset nor direct access to the dataset is available.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="In the GitHub repo, authors mention that “no pre-trained models are provided here”.">✘</a></td><td class="partial data-cell"><a href="https://github.com/archinetai/audio-diffusion-pytorch/blob/main/README.md" target="_blank" title="Code is partially describe. How to use Moûsai is not straightforward described.">~</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2301.11757" target="_blank" title="Training procedure is describe in the article, including hardware requirements and model configuration.">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2301.11757" target="_blank" title="Evaluation data or code for evaluation are not specified.">~</a></td><td class="open data-cell"><a href="https://aclanthology.org/2024.acl-long.437/" target="_blank" title="Accepted at ACL 2024.">✔︎</a></td><td class="open data-cell"><a href="https://github.com/archinetai/audio-diffusion-pytorch/blob/main/LICENSE" target="_blank" title="Code is under the MIT license.">✔︎</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="star data-cell"><a href="https://pypi.org/project/audio-diffusion-pytorch/" target="_blank" title="Code belongs to https://github.com/archinetai/audio-diffusion-pytorch library.">⭐</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="star data-cell"><a href="https://diligent-pansy-4cb.notion.site/Music-Generation-with-Diffusion-ebe6e9e528984fa1b226d408f6002d87" target="_blank" title="Supplementary material is available.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/archinetai/audio-diffusion-pytorch" target="_blank" title=""><span style="display:inline-block;min-width:36px">2023</span>ETH Zürich, IIT Kharagpur, Max Planck Institute</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="ETH Zürich, IIT Kharagpur, Max Planck Institute"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/diff-a-riff.yaml" target="_blank" title="data: diff-a-riff.yaml">Diff-A-Riff</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Dataset is closed and only briefly described.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code.">✘</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2406.08384" target="_blank" title="Training procedure is properly described.">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2406.08384" target="_blank" title="Missing data and metrics implementations.">~</a></td><td class="open data-cell"><a href="https://ismir2024program.ismir.net/poster_225.html" target="_blank" title="Accepted at ISMIR 2024.">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not available.">✘</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="star data-cell"><a href="https://sonycslparis.github.io/diffariff-companion/" target="_blank" title="A demo page with sound examples and demonstration of the models capabilities is available.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://sonycslparis.github.io/diffariff-companion/" target="_blank" title=""><span style="display:inline-block;min-width:36px">2024</span>Sony Computer Science Laboratories Paris and Queen Mary University of London</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Sony Computer Science Laboratories Paris and Queen Mary University of London"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/music-controlnet.yaml" target="_blank" title="data: music-controlnet.yaml">Music ControlNet</a></td><td class="closed data-cell"><a href="" target="_blank" title="No official code repository available.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Very limited description of the data used for training, and no sources provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No model weights provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code available so no documentation.">✘</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2311.07069" target="_blank" title="Training procedure is described in detail in the paper.">✔︎</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2311.07069" target="_blank" title="Evaluation procedure is described, but details on the specific implementation used for some of the evaluation metrics are not given, limiting the reproducibility of results.">~</a></td><td class="open data-cell"><a href="https://dl.acm.org/doi/10.1109/TASLP.2024.3399026" target="_blank" title="Accepted at IEEE/ACM Transactions on Audio, Speech, and Laguange Processing (open access).">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not available.">✘</a></td><td class="∅ data-cell"><a href="" target="_blank" title="No model card available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="No datasheet available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="No package available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="No user interface nor real-time implementation available.">∅</a></td><td class="star data-cell"><a href="https://musiccontrolnet.github.io/web/" target="_blank" title="A complementary website with sound examples and demo of the model is available.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://musiccontrolnet.github.io/web/" target="_blank" title=""><span style="display:inline-block;min-width:36px">2023</span>Carnegie Mellon University and Adobe Research</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Carnegie Mellon University and Adobe Research"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/musiclm.yaml" target="_blank" title="data: musiclm.yaml">MusicLM</a></td><td class="closed data-cell"><a href="" target="_blank" title="Source code not available.">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="Training data is not available nor properly described.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not available.">✘</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2301.11325" target="_blank" title="Training procedure is partially documented, missing the hardware requirements.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2301.11325" target="_blank" title="Evaluation data is provided and metrics are described, but exact implementations are not referenced.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2301.11325" target="_blank" title="Peer-reviewed paper not found.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not available.">✘</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="star data-cell"><a href="https://google-research.github.io/seanet/musiclm/examples/" target="_blank" title="Complementary page is available with sound examples and demonstration of the model's capabilities.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://musiclm.com/" target="_blank" title=""><span style="display:inline-block;min-width:36px">2023</span>Google Research and IRCAM</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Google Research and IRCAM"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/noise2music.yaml" target="_blank" title="data: noise2music.yaml">Noise2Music</a></td><td class="closed data-cell"><a href="Not available." target="_blank" title="No source codebase is available.">✘</a></td><td class="partial data-cell"><a href="" target="_blank" title="Training data is described in detail. However, the resulting MuLaMCap dataset resulting from this work is not publicly available.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Model weights are not available.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No codebase is available. There is no documentation available either.">✘</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2302.03917" target="_blank" title="The available pre-print describes partially the training procedure of the model. While some relevant information is described, such as model configuration and training details, there are key aspects of the training missing, such as hardware requirements, model checkpoints and hyperparameters.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2302.03917" target="_blank" title="Despite the evaluation procedure is well-documented in regards to evaluation data and metrics, the evaluation process lacks some relevant details to ensure replicability. Moreover, evaluation code of the system is not available.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/abs/2302.03917" target="_blank" title="Preprint is available. No peer-reviewed version of the article has been found.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not available.">✘</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="">∅</a></td><td class="star data-cell"><a href="https://google-research.github.io/noise2music/" target="_blank" title="Complementary site with sound examples is available.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="Not available." target="_blank" title=""><span style="display:inline-block;min-width:36px">2023</span>Google Research</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Google Research"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/ditto-2.yaml" target="_blank" title="data: ditto-2.yaml">DITTO-2</a></td><td class="closed data-cell"><a href="" target="_blank" title="No code repository.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Training data neither available nor properly described.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No code repository.">✘</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2405.20289" target="_blank" title="Training procedure is partially described.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2405.20289" target="_blank" title="Metrics and data are described but exact implementations are missing.">~</a></td><td class="open data-cell"><a href="https://ismir2024program.ismir.net/poster_146.html" target="_blank" title="Accepted at ISMIR 2024">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not available.">✘</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not available.">∅</a></td><td class="star data-cell"><a href="https://ditto-music.github.io/ditto2/" target="_blank" title="Complementary demo page is available, including sound examples of the model's capabilities.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://ditto-music.github.io/ditto2/" target="_blank" title=""><span style="display:inline-block;min-width:36px">2024</span>University of California San Diego and Adobe Research</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="University of California San Diego and Adobe Research"></a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/melody.yaml" target="_blank" title="data: melody.yaml">MeLoDy</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not provided and only briefly described.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not provided.">✘</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code.">✘</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2305.15719" target="_blank" title="Training procedure partially described.">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2305.15719" target="_blank" title="Missing information about data and specific implementations.">~</a></td><td class="open data-cell"><a href="https://dl.acm.org/doi/10.5555/3666122.3666888" target="_blank" title="Accepted at NeurIPS 2023.">✔︎</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not available.">✘</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not provided.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not provided.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not provided.">∅</a></td><td class="∅ data-cell"><a href="" target="_blank" title="Not provided.">∅</a></td><td class="star data-cell"><a href="https://efficient-melody.github.io/#" target="_blank" title="A complementary web page with sound examples and demonstration of the model's capabilities is available.">⭐</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://efficient-melody.github.io/" target="_blank" title=""><span style="display:inline-block;min-width:36px">2023</span>ByteDance</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="ByteDance"></a></td></tr>
</tbody>
</table>
</div>
<h3>Key Findings</h3>
<ul>
<li>The leaderboard reveals substantial variation in openness across music-generative models, with training procedure often most open, while training data remains the most closed.</li>
<li>Models that share source code, model weights, and documentation typically also apply open or responsible licenses, indicating a correlation across these core categories.</li>
<li>Desirable categories like datasheets are less frequently fulfilled, while supplementary material pages with demos are becoming a community norm.</li>
<li>The leaderboard helps identify incomplete openness claims and potential ‘open-washing’, providing clear, evidence-based signals for transparency and accountability in the field.</li>
</ul>
<h3>Limitations</h3>
<ul>
<li>In music, assessing training data openness is challenging due to IP constraints, and fully open status may rely on detailed documentation rather than direct data release.</li>
<li>Hardware implications are underexplored: while some models can be trained on personal computers, others require heavy computational resources, affecting reproducibility and accessibility.</li>
<li>The leaderboard does not capture ethical, societal, or creative impacts of these models, focusing strictly on openness dimensions. Yet, it does provide a foundation upon which these critical aspects can be integrated in future iterations.</li>
</ul>
<div class="highlight" id="disclaimer">
<p><strong>Disclaimer: Future Developments 🚧</strong> </p>
<p>The MusGO framework is a living resource, developed through community collaboration, currently focused on assessing openness in music-generative AI. 
  However, we are actively exploring complementary perspectives and refinements to further expand its scope and adaptability. 
  We aim to better reflect the diverse ways in which music-generative systems can be understood, accessed, and used responsibly. <br/><br/>
  Updates will be shared once ready for community feedback. </p>
</div>
<h3>Acknowledgments</h3>
<p>This site is an adapted version of <a href="https://opening-up-chatgpt.github.io/">https://opening-up-chatgpt.github.io/</a>.  
  We are deeply grateful to the original creators, Andreas Liesenfeld, Alianda Lopez, and Mark Dingemanse, for their groundbreaking work on openness, transparency, and accountability in generative AI, which has inspired and shaped this project.</p>
<p>For more details, please refer to their papers:</p>
<ul>
<li>
      Liesenfeld, Andreas, Alianda Lopez, and Mark Dingemanse. 2023. 
      <em>“Opening up ChatGPT: Tracking Openness, Transparency, and Accountability in Instruction-Tuned Text Generators.”</em> 
      In <em>CUI '23: Proceedings of the 5th International Conference on Conversational User Interfaces</em>, July 19–21, Eindhoven.  
      DOI: <a href="https://doi.org/10.1145/3571884.3604316" target="_blank">10.1145/3571884.3604316</a>.
    </li>
<li>
      Andreas Liesenfeld and Mark Dingemanse. 2024. 
      <em>“Rethinking open source generative AI: open washing and the EU AI Act.”</em> 
      In <em>The 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24)</em>.  
      DOI: <a href="https://doi.org/10.1145/3630106.3659005" target="_blank">10.1145/3630106.3659005</a>.
    </li>
</ul>

  This work has been supported by <em>IA y Música: Cátedra
en Inteligencia Artificial y Música</em> (TSI-100929-2023-1),
funded by the Secretaría de Estado de Digitalización e Inteligencia
Artificial and the European Union-Next Generation
EU, and <em>IMPA: Multimodal AI for Audio Processing</em> 
(PID2023-152250OB-I00), funded by the Ministry of Science,
Innovation and Universities of the Spanish Government,
the Agencia Estatal de Investigación (AEI) and cofinanced
by the European Union. <br/><br/>

We thank our colleagues
at the Music Technology Group at Universitat Pompeu
Fabra for their thoughtful insights, constructive discussions
and active engagement throughout the development
of this work.

</div><!-- #content -->
<div id="footer">
<div class="footer-content">
<div class="footer-text">
<p>This site is an adapted version of <a href="https://opening-up-chatgpt.github.io/">https://opening-up-chatgpt.github.io/</a>.</p>
<p class="copyright">Website &amp; code © 2025 by the authors.</p>
<p id="build-time">Table last built on 2025-12-16 at 11:42 UTC</p>
</div>
<div class="footer-image">
<img alt="MTG logo" src="fig/MTG-logo.png"/>
</div>
</div>
</div>
</body>
</html>
