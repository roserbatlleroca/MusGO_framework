<!DOCTYPE html>

<html>
<head>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>MusGO Framework: Assessing Opennesss in Music-Generative AI</title>
<link href="styles.css" rel="stylesheet"/>
<!--<link href="favicon.png" rel="icon" type="image/x-icon"/>-->
<script data-domain="roserbatlleroca.github.io/MusGO_framework/" defer="" src="https://plausible.io/js/script.js"></script>
</head>
<body>
<div id="header">
<h1><a href="" title="MusGO Framework: Assessing Opennesss in Music-Generative AI">MusGO Framework:</a> Assessing Opennesss in Music-Generative AI</h1><!--<img alt="Opening up ChatGPT" id="title-logo" src="logos/openchatgpt-logo-favicon-red-on-transparent.png"/>Opening up ChatGPT</a>: tracking openness of instruction-tuned LLMs</h1>-->
</div>
<div id="content">
<!--<p class="highlight" id="citation">âš¡<strong>FAccT'24 paper</strong>âš¡ Liesenfeld, Andreas, and Mark Dingemanse. 2024. â€˜Rethinking Open Source Generative AI: Open-Washing and the EU AI Actâ€™. In <em>The 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT â€™24)</em>. Rio de Janeiro, Brazil: ACM. (<a href="https://pure.mpg.de/pubman/item/item_3588217_2/component/file_3588218/liesenfeld_dingemanse_2024_FAccT_generative_AI_open-washing_EU_AI_Act.pdf" target="_blank">PDF</a>).</p>-->
<p class="highlight" id="description">Introducing the <strong>MusGO Framework</strong>: A community-driven framework for assessing openness in music-generative AI.</p>
<!--<p id="tagline">There is a growing amount of instruction-tuned text generators billing themselves as 'open source'. How open are they really? <span class="link-icon">ğŸ”—</span><a href="https://dl.acm.org/doi/10.1145/3630106.3659005" target="_blank">FAccT'24</a> <span class="link-icon">ğŸ”—</span><a href="https://doi.org/10.1145/3571884.3604316" target="_blank">CUI'23</a> <span class="link-icon">ğŸ”—</span><a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/">repo</a></p>-->
<p id="tagline">With the notable rise of generative models in the music domain, we pose the question: how open are they? MusGO is built and applied to assess openness in music-generative models. Note that this site is part of the submission <em>'MusGO: A Community-Driven Framework for Asssessing Openness in Music-Generative AI'</em> at ACM Conference on Fairness, Accountability, and Transparency (FAccT '25), which is currently under review.</p>
<h3>Openness assessment</h3>
<div id="included-table"><table>
<thead>
<tr class="main-header"><th>Project</th><th colspan="8">Essential</th><th colspan="5">Nice-to-have</th>
<tr class="second-header"><th></th><th>Source code</th><th>Training data</th><th>Model weights</th><th>Code<br/>documentation</th><th>Training<br/>procedure</th><th>Evaluation<br/>procedure</th><th>Research paper</th><th>License</th><th>Model card</th><th>Datasheet</th><th>Package</th><th>UX<br/>application</th><th>Supplementary<br/>material page</th>
</tr></tr></thead>
<tbody>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/musicgen.yaml" target="_blank" title="data: musicgen.yaml">MusicGen</a></td><td class="open data-cell"><a href="https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md" target="_blank" title="Code is provided in https://github.com/facebookresearch/audiocraft/ (Training and inference)">âœ”ï¸</a></td><td class="partial data-cell"><a href="" target="_blank" title="â€œWe rely on an internal dataset of 10K high-quality music tracks, and on the ShutterStock and Pond5 music data collections2 with respectively 25K and 365K instrument-only music tracks.â€ â€œNote that we do NOT provide any of the datasets used for training MusicGen. We provide a dummy dataset containing just a few examples for illustrative purposes.â€">~</a></td><td class="open data-cell"><a href="" target="_blank" title="Models are available in the GitHub repository and in Hugging Face.">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Training code is available and seem well documented: https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md Warning: Since data is not available is not easy to replicate.">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Training is well documented. https://github.com/facebookresearch/audiocraft/blob/main/docs/TRAINING.md with training pipelines and environment setup described. Note: Hardware use for training is described in the paper. ">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="">âœ”ï¸</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2306.05284" target="_blank" title="Accepted at NeurIPS 2023">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="The code in this repository is released under the MIT license as found in the https://github.com/facebookresearch/audiocraft/blob/main/LICENSE. The models weights in this repository are released under the CC-BY-NC 4.0 license as found in the https://github.com/facebookresearch/audiocraft/blob/main/LICENSE_weights.">âœ”ï¸</a></td><td class="star data-cell"><a href="" target="_blank" title="Model card discusses intended use, limitations and biases. https://github.com/facebookresearch/audiocraft/blob/main/model_cards/MUSICGEN_MODEL_CARD.md">â­</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Data is copyrighted and not available.">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="Has two packages: audiocraft package from Meta, and transformers package from Hugging Face. ">â­</a></td><td class="star data-cell"><a href="" target="_blank" title="Demo in Hugging Face spaces, user-oriented gradio demo, and jupyter notebooks. https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md#usage">â­</a></td><td class="star data-cell"><a href="" target="_blank" title="Supplementary material web page with sound examples. https://ai.honu.io/papers/musicgen/">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/facebookresearch/audiocraft/blob/main/docs/MUSICGEN.md" target="_blank" title="">Meta AI</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Meta AI">94</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/jasco.yaml" target="_blank" title="data: jasco.yaml">JASCO</a></td><td class="open data-cell"><a href="https://github.com/facebookresearch/audiocraft/blob/main/docs/JASCO.md" target="_blank" title="Code is avialble at https://github.com/facebookresearch/audiocraft/blob/main/docs/JASCO.md">âœ”ï¸</a></td><td class="partial data-cell"><a href="" target="_blank" title="They describe the datasets used to training their model. However, there is a 10k songs set that is not described (proprietary data). ">~</a></td><td class="open data-cell"><a href="" target="_blank" title="Pretrained models are accessible via API ">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Code is documented in GitHub repo, including installation requirements. ">âœ”ï¸</a></td><td class="partial data-cell"><a href="" target="_blank" title="Training procedure is described in the research paper, but hardware requirements are not mentioned. ">~</a></td><td class="open data-cell"><a href="" target="_blank" title="">âœ”ï¸</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2406.10970" target="_blank" title="Accepted at ISMIR 2024. ">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title=" Code is released under MIT, model weights are released under CC-BY-NC 4.0.">âœ”ï¸</a></td><td class="star data-cell"><a href="" target="_blank" title="https://github.com/facebookresearch/audiocraft/blob/main/model_cards/JASCO_MODEL_CARD.md">â­</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="Installable through audiocraft pip package">â­</a></td><td class="star data-cell"><a href="" target="_blank" title="API provided via Hugging Face. ">â­</a></td><td class="star data-cell"><a href="" target="_blank" title="https://pages.cs.huji.ac.il/adiyoss-lab/JASCO/">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/facebookresearch/audiocraft/blob/main/docs/JASCO.md" target="_blank" title="">The Hebrew University of Jerusalem, Meta AI</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="The Hebrew University of Jerusalem, Meta AI">88</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/stable audio open.yaml" target="_blank" title="data: stable audio open.yaml">Stable Audio Open</a></td><td class="open data-cell"><a href="https://huggingface.co/stabilityai/stable-audio-open-1.0" target="_blank" title=" Code for data processing, training pipeline, and inference is available in the stable-audio-tools repository. Architecture of the mode is specified in the form of a config file.https://github.com/Stability-AI/stable-audio-tools">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Training data can be mapped from Attribution files.https://huggingface.co/stabilityai/stable-audio-open-1.0/blob/main/fma_dataset_attribution2.csv. However, accessibility is subject to accepting Stability AI conditions. ">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Weights are available at: https://huggingface.co/stabilityai/stable-audio-open-1.0. However, accessibility is subject to accepting Stability AI conditions. ">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Documentation of the code is limited for training replicating the model (not specific config files, but those from Stable Audio could be used). However, I think we can still consider the documentation of the code to be enough.https://huggingface.co/stabilityai/stable-audio-open-1.0https://github.com/Stability-AI/stable-audio-tools">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Described in pre-print. Warning: They say they provide the exact parameters for training in the repository, but I canâ€™t find them. ">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="">âœ”ï¸</a></td><td class="partial data-cell"><a href="http://arxiv.org/abs/2407.14358" target="_blank" title="Paper is in arxiv but it is not peer-reviewed. ">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="Stability AI Community Licensehttps://huggingface.co/stabilityai/stable-audio-open-1.0/blob/main/LICENSE.mdNot an Open Source Initiate(OSI) approved licences ">~</a></td><td class="star data-cell"><a href="" target="_blank" title="Model card in HuggingFace: https://huggingface.co/stabilityai/stable-audio-open-1.0">â­</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="They provide a .csv file listing each wav from Freesound and FMA. But there is no proper datasheet. Itâ€™s a pity they donâ€™t get this start because they are careful about the data they use.    ">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="The model can be used with: 1. the https://github.com/Stability-AI/stable-audio-tools library and 2. the https://huggingface.co/docs/diffusers/main/en/index library">â­</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="I canâ€™t find an official user-oriented application apart from the packages and code examples. But I am sure that the community around the model is building this kind of resource. We should check.">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="They provide sound examples in https://stability.ai/news/introducing-stable-audio-open">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://huggingface.co/stabilityai/stable-audio-open-1.0" target="_blank" title="">Stability AI</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Stability AI">88</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/gansynth.yaml" target="_blank" title="data: gansynth.yaml">GANsynth</a></td><td class="open data-cell"><a href="https://github.com/magenta/magenta/tree/main/magenta/models/gansynth" target="_blank" title="Source code is available.">âœ”ï¸</a></td><td class="partial data-cell"><a href="" target="_blank" title="Dataset is available, but the reduced version that they use and the newly created test/train splits are not provided.">~</a></td><td class="open data-cell"><a href="" target="_blank" title="Two pretrained checkpoints are provided.">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Codebase is documented, including high-level instructions and configuration files.">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Training procedure is fully documented, including hardware requirements and model configuration.">âœ”ï¸</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/1902.08710" target="_blank" title="Accepted at ICLR 2019.">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="System is covered by Apache License. ">âœ”ï¸</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Very few details are given of the pretrained checkpoints.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available.">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="Available within the magenta pip package.">â­</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available.">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="https://storage.googleapis.com/magentadata/papers/gansynth/index.html">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/magenta/magenta/tree/main/magenta/models/gansynth" target="_blank" title="">Google Magenta</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Google Magenta">88</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/musika.yaml" target="_blank" title="data: musika.yaml">Musika</a></td><td class="open data-cell"><a href="https://github.com/marcoppasini/musika" target="_blank" title="Codebase is available and complete. ">âœ”ï¸</a></td><td class="partial data-cell"><a href="" target="_blank" title="Training data is described in the research paper. Some of the data used is publicly available: LibriTTS corpus. However, there are some sections of the used data that are not accessible or detailed enough to fully reconstruct the dataset:  South by SouthWest. Also, music coming from http://jamendo.com under â€œtechnoâ€ genre, it not detailed. ">~</a></td><td class="open data-cell"><a href="" target="_blank" title="Weights are publicly available. ">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Codebase is documented, including details on environment and configuration settings. ">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Detailed in research paper and proper instructions are given in the model repository. ">âœ”ï¸</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="open data-cell"><a href="" target="_blank" title="Accepted at ISMIR 2022.">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="MIT License. ">âœ”ï¸</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available. ">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available. ">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available. Source code is provided through GitHub, but not as an installable packaged or with version control. ">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="An interface is provided through Gradio. In addition, there is a colab notebook that intends to ease accessibility for non technical users: https://colab.research.google.com/drive/1PowSw3doBURwLE-OTCiWkO8HVbS5paRb">â­</a></td><td class="star data-cell"><a href="" target="_blank" title="https://marcoppasini.github.io/musika">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/marcoppasini/musika" target="_blank" title="">Johannes Kepler University Linz</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Johannes Kepler University Linz">88</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/musicldm.yaml" target="_blank" title="data: musicldm.yaml">MusicLDM</a></td><td class="partial data-cell"><a href="https://github.com/RetroCirce/MusicLDM/?tab=readme-ov-file" target="_blank" title="System source code is available at GitHub repository &amp; Hugging Face Diffusers. ">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="MusicLDM is trained on the Audiostock dataset, which contains 9000 music tracks for training and 1000 tracks for testing. Dataset is not directly provided, no information about the original sources accessibility or requirements is provided. ">~</a></td><td class="open data-cell"><a href="" target="_blank" title="Model checkpoints are available here: https://drive.google.com/drive/folders/15VDVcIgf99YRM5oGXhRxa_Rowl54uWho?usp=sharing">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Description on how to use the code is provided in the GitHub repo and through Hugging Face Diffusers. ">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Training procedure is documented in the research article preprint and in the appendix additional page for the peer-reviewed version: https://musicldm.github.io/appendix/. Authors include specifications on hyperparameters, GPU requirements and model configurations. ">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="">âœ”ï¸</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2308.01546" target="_blank" title="This article has been accepted at ICASSP 2024. The access of such paper is limited to IEEE Xplore access: https://ieeexplore.ieee.org/document/10447265. A preprint is available in arXiv: https://arxiv.org/pdf/2308.01546">âœ”ï¸</a></td><td class="partial data-cell"><a href="" target="_blank" title="Attribution-NonCommercial-ShareAlike 4.0 International">~</a></td><td class="star data-cell"><a href="" target="_blank" title="Available at HuggingFace: https://huggingface.co/ucsd-reach/musicldm Information about modelâ€™s evaluation and limitations is missing and modelâ€™s architecture is just mentioned.  ">â­</a></td><td class="star data-cell"><a href="" target="_blank" title="There is a data card: https://github.com/LAION-AI/audio-dataset/blob/main/data_card/Audiostock.md. However, content is limited to data sources origin and details on how to reproduce the data collection. Details on curation and other considerations, such as consent, limitations and selection strategies are missing. ">â­</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="API is shut down temporarily. Should be released again soon. ">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="https://musicldm.github.io/">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/RetroCirce/MusicLDM/?tab=readme-ov-file" target="_blank" title="">University of California San Diego,Â Mila-Quebec Artificial Intelligence Institute, University of Surrey,Â LAION</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="University of California San Diego,Â Mila-Quebec Artificial Intelligence Institute, University of Surrey,Â LAION">81</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/vampnet.yaml" target="_blank" title="data: vampnet.yaml">VampNet</a></td><td class="open data-cell"><a href="https://github.com/hugofloresgarcia/vampnet" target="_blank" title="System source code is available and accessible at https://github.com/hugofloresgarcia/vampnet.">âœ”ï¸</a></td><td class="closed data-cell"><a href="" target="_blank" title="Dataset is not available nor is completely described. ">âœ˜</a></td><td class="open data-cell"><a href="" target="_blank" title="Model checkpoints are available https://zenodo.org/records/8136629. The weights for the models are licensedÂ https://creativecommons.org/licenses/by-nc-sa/4.0/deed.ml">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Codebase is documented. ">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Described in the article, with multiple details on architecture, hyperparameters, GPU usage">âœ”ï¸</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2307.04686" target="_blank" title="Article peer-reviewed &amp; accepted at ISMIR 2023: https://archives.ismir.net/ismir2023/paper/000042.pdf">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="MIT License">âœ”ï¸</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="Gradio UI is provided in the codebase. ">â­</a></td><td class="star data-cell"><a href="" target="_blank" title="Untitled (https://www.notion.so/e37aabd0d5f1493aa42c5711d0764b33?pvs=21) ">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/hugofloresgarcia/vampnet" target="_blank" title="">Northwestern UniversityÂ andÂ Descript Inc</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Northwestern UniversityÂ andÂ Descript Inc">81</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/rave.yaml" target="_blank" title="data: rave.yaml">RAVE</a></td><td class="open data-cell"><a href="https://github.com/acids-ircam/RAVE" target="_blank" title="Source code is available, including data preparation, model architecture, training pipeline and inference.">âœ”ï¸</a></td><td class="closed data-cell"><a href="" target="_blank" title="Training data is internal and just briefly described. ">âœ˜</a></td><td class="open data-cell"><a href="" target="_blank" title="Several pretrained models are provided.">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Code is thoroughly documented, allowing for understanding and reproducibility of the model.">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Training procedure is described.">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="">âœ”ï¸</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2111.05011" target="_blank" title="Rejected at ICLR 2022. No peer-reviewed version. ">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="System covered by an CC BY-NC 4.0 license.">~</a></td><td class="star data-cell"><a href="" target="_blank" title="https://acids-ircam.github.io/rave_models_download">â­</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available.">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="Pip package is available.">â­</a></td><td class="star data-cell"><a href="" target="_blank" title="Real-time implementation of the model and tutorials are provided.">â­</a></td><td class="star data-cell"><a href="" target="_blank" title="https://anonymous84654.github.io/RAVE_anonymous/">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/acids-ircam/RAVE" target="_blank" title="">IRCAM</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="IRCAM">75</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/jukebox.yaml" target="_blank" title="data: jukebox.yaml">Jukebox</a></td><td class="open data-cell"><a href="https://github.com/openai/jukebox" target="_blank" title="Code is available at https://github.com/openai/jukebox">âœ”ï¸</a></td><td class="partial data-cell"><a href="" target="_blank" title="Training data is partially described in the preprint article. However, relevant information is missing, such as specific sources, and data itself cannot be accessed â€” dataset is not publicly available. ">~</a></td><td class="open data-cell"><a href="" target="_blank" title="Weights are available.">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Codebase is documented. ">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Information available between article and GitHub repository.  ">âœ”ï¸</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2005.00341" target="_blank" title="Only preprint of the research paper is available. ">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="Codebase is licensed under a non-commercial use license by OpenAI. ">~</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available. ">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available. ">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available. Codebase is officially not mantained. ">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="A demo/summary page is available: https://openai.com/index/jukebox/However, sonifications of the examples generations are not available anymore. There exist another site with generation examples, but most of them are not available https://jukebox.openai.com/. In addition, a colab notebook is provided: https://colab.research.google.com/github/openai/jukebox/blob/master/jukebox/Interacting_with_Jukebox.ipynb">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/openai/jukebox" target="_blank" title="">OpenAI</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="OpenAI">75</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/moÃ»sai.yaml" target="_blank" title="data: moÃ»sai.yaml">MoÃ»sai</a></td><td class="partial data-cell"><a href="https://github.com/archinetai/audio-diffusion-pytorch" target="_blank" title="They provide an audio diffusion library that includes different models. However, the configs shown are indicative and untested, seeÂ https://arxiv.org/abs/2301.11757Â for the configs used in the paper.">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="How the data is collected and acquired, including licensing issues, is detailed in the research paper. However, the exact list of songs in the dataset nor direct access to the dataset is available. ">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="In the GitHub repo, authors mention that â€œno pre-trained models are provided hereâ€. ">âœ˜</a></td><td class="partial data-cell"><a href="" target="_blank" title="Code is partially describe. How to use MoÃ»sai is not straightforward described.">~</a></td><td class="open data-cell"><a href="" target="_blank" title="Training procedure is describe in the article, including hardware requirements and model configuration. ">âœ”ï¸</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2301.11757" target="_blank" title="Accepted at ACL 2024">âœ”ï¸</a></td><td class="open data-cell"><a href="" target="_blank" title="Code is under the MIT license. ">âœ”ï¸</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="Code belongs to https://github.com/archinetai/audio-diffusion-pytorch library">â­</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="Supplementary material is available at Untitled (https://www.notion.so/ebe6e9e528984fa1b226d408f6002d87?pvs=21) ">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="https://github.com/archinetai/audio-diffusion-pytorch" target="_blank" title="">ETH ZÃ¼rich,Â IIT Kharagpur,Â Max Planck Institute</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="ETH ZÃ¼rich,Â IIT Kharagpur,Â Max Planck Institute">62</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/diff-a-riff.yaml" target="_blank" title="data: diff-a-riff.yaml">Diff-A-Riff</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code.">âœ˜</a></td><td class="closed data-cell"><a href="" target="_blank" title="Dataset is closed and only briefly described. ">âœ˜</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not provided.">âœ˜</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code.">âœ˜</a></td><td class="open data-cell"><a href="" target="_blank" title="Training procedure is properly described.">âœ”ï¸</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2406.08384" target="_blank" title="Accepted at ISMIR 2024">âœ”ï¸</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not available.">âœ˜</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available.">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="https://sonycslparis.github.io/diffariff-companion/">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="" target="_blank" title="">Sony Computer Science Laboratories ParisÂ andÂ Queen Mary University of London</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Sony Computer Science Laboratories ParisÂ andÂ Queen Mary University of London">31</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/music controlnet.yaml" target="_blank" title="data: music controlnet.yaml">Music ControlNet</a></td><td class="closed data-cell"><a href="" target="_blank" title="No official code repository available.">âœ˜</a></td><td class="closed data-cell"><a href="" target="_blank" title="Very limited description of the data used for training, and no sources provided.">âœ˜</a></td><td class="closed data-cell"><a href="" target="_blank" title="No model weights provided.">âœ˜</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code available so no documentation.">âœ˜</a></td><td class="open data-cell"><a href="" target="_blank" title="Training procedure is described in detail in the paper.">âœ”ï¸</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2311.07069" target="_blank" title="A peer-reviewed paper is available and publicly accessible.">âœ”ï¸</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not available.">âœ˜</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="No model card available.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="No datasheet available.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="No package available.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="No user interface nor real-time implementation available.">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="https://musiccontrolnet.github.io/web/">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="" target="_blank" title="">Carnegie Mellon UniversityÂ andÂ Adobe Research</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Carnegie Mellon UniversityÂ andÂ Adobe Research">31</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/ditto-2.yaml" target="_blank" title="data: ditto-2.yaml">DITTO-2</a></td><td class="closed data-cell"><a href="" target="_blank" title="No code repository.">âœ˜</a></td><td class="closed data-cell"><a href="" target="_blank" title="Training data neither available nor properly described.">âœ˜</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not provided.">âœ˜</a></td><td class="closed data-cell"><a href="" target="_blank" title="No code repository.">âœ˜</a></td><td class="partial data-cell"><a href="" target="_blank" title="Training procedure is partially described.">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="open data-cell"><a href="https://arxiv.org/abs/2405.20289" target="_blank" title="Accepted at ISMIR 2024">âœ”ï¸</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not available.">âœ˜</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available.">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="https://ditto-music.github.io/ditto2/">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="" target="_blank" title="">University of California San DiegoÂ andÂ Adobe Research</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="University of California San DiegoÂ andÂ Adobe Research">25</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/melody.yaml" target="_blank" title="data: melody.yaml">MeLoDy</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code.">âœ˜</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not provided and only briefly described.">âœ˜</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not provided.">âœ˜</a></td><td class="closed data-cell"><a href="" target="_blank" title="No source code.">âœ˜</a></td><td class="partial data-cell"><a href="" target="_blank" title="Training procedure partially described.">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="open data-cell"><a href="https://arxiv.org/pdf/2305.15719" target="_blank" title="Accepted at NeurIPS 2023: https://dl.acm.org/doi/10.5555/3666122.3666888">âœ”ï¸</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not available.">âœ˜</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not provided.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not provided.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not provided.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not provided.">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="https://efficient-melody.github.io/#">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="" target="_blank" title="">ByteDance</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="ByteDance">25</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/musiclm.yaml" target="_blank" title="data: musiclm.yaml">MusicLM</a></td><td class="closed data-cell"><a href="" target="_blank" title="Source code not available.">âœ˜</a></td><td class="partial data-cell"><a href="" target="_blank" title="Training data is not available nor properly described.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not provided.">âœ˜</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not available.">âœ˜</a></td><td class="partial data-cell"><a href="" target="_blank" title="Training procedure is partially documented, missing the hardware requirements.">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2301.11325" target="_blank" title="Peer-reviewed paper not found.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not available.">âœ˜</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available.">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="In theory available to try in AI Test Kitchen but I never managed?">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="https://google-research.github.io/seanet/musiclm/examples/">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="" target="_blank" title="">Google Research and IRCAM</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Google Research and IRCAM">25</a></td></tr>
<tr class="row-a"><td class="name-cell"><a href="https://github.com/roserbatlleroca/MusGO_framework/blob/main/projects/noise2music.yaml" target="_blank" title="data: noise2music.yaml">Noise2Music</a></td><td class="closed data-cell"><a href="Not available. " target="_blank" title="No source codebase is available. ">âœ˜</a></td><td class="partial data-cell"><a href="" target="_blank" title="Training data is described in detail. However, the resulting MuLaMCap dataset resulting from this work is not publicly available.">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Model weights are not available. ">âœ˜</a></td><td class="closed data-cell"><a href="" target="_blank" title="No codebase is available. There is no documentation available either. ">âœ˜</a></td><td class="partial data-cell"><a href="" target="_blank" title="The available preprint describes partially the training procedure of the model. While some relevant information is described, such as model configuration and training details, there are key aspects of the training missing, such as hardware requirements, model checkpoints and hyperparameters. ">~</a></td><td class="partial data-cell"><a href="" target="_blank" title="">~</a></td><td class="partial data-cell"><a href="https://arxiv.org/pdf/2302.03917" target="_blank" title="Preprint is available. No peer-reviewed version of the article has been found. ">~</a></td><td class="closed data-cell"><a href="" target="_blank" title="Not available. ">âœ˜</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available. ">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available. ">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="Not available. ">âˆ…</a></td><td class="âˆ… data-cell"><a href="" target="_blank" title="">âˆ…</a></td><td class="star data-cell"><a href="" target="_blank" title="https://google-research.github.io/noise2music/">â­</a></td></tr>
<tr class="row-b"><td class="org"><a href="Not available. " target="_blank" title="">Google Research</a></td><td colspan="7"></td><td class="source-link"><a href="" target="_blank" title="Google Research">25</a></td></tr>
</tbody>
</table>
</div>
<p id="table-guide"><em>How to interpret this table?</em> MusGO framework is composed of 13 different dimensions of openness. It distinguishes between <em>essential</em> (1-8) and <em>nice-to-have</em> (9-13) categories. 
	Essential categories follow a three-level scale: (<span class="openness open"><strong>âœ”ï¸</strong> open</span>, <span class="openness partial"><strong>~</strong> partial</span> or <span class="openness closed"><strong>âœ˜</strong> closed</span>). <!-- with a direct link to the available evidence; on hover, the cell will display the notes we have on file for that judgement. The name of each project is a direct link to source data.-->
	Instrad, nice-to-have categories are binary: whether that element exists (â­) or not. The openness level is computed with the essential categories punctuations, where <strong>âœ”ï¸</strong> is 1, <strong>~</strong> is 0.5 and <strong>âœ˜</strong> is 0 points. Openness level is normalised to obtained a value on a 100 point scale. 
	Models are arranged in descending order of openness level, placing the <strong>most open model</strong> first.</p>
<h4>Acknowledgments</h4>
<p>This site is an adapted version of <a href="https://opening-up-chatgpt.github.io/">https://opening-up-chatgpt.github.io/</a>.
 We are deeply grateful to the original creators, Andreas Liesenfeld, Alianda Lopez, and Mark Dingemanse, for their groundbreaking work on openness, transparency, and accountability in generative AI, which has inspired and shaped this project.</p>
<p>
    For more details, please refer to their papers:
  </p>
<ul>
<li>
      Liesenfeld, Andreas, Alianda Lopez, and Mark Dingemanse. 2023. 
      <em>â€œOpening up ChatGPT: Tracking Openness, Transparency, and Accountability in Instruction-Tuned Text Generators.â€</em> 
      In <em>CUI '23: Proceedings of the 5th International Conference on Conversational User Interfaces</em>, July 19-21, Eindhoven. 
      DOI: <a href="https://doi.org/10.1145/3571884.3604316" target="_blank">10.1145/3571884.3604316</a>.
    </li>
<li>
      Andreas Liesenfeld and Mark Dingemanse. 2024. 
      <em>â€œRethinking open source generative AI: open washing and the EU AI Act.â€</em> 
      In <em>The 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24)</em>. 
      DOI: <a href="https://doi.org/10.1145/3630106.3659005" target="_blank">10.1145/3630106.3659005</a>.
    </li>
</ul>
<!-- <h2>Why is openness important?</h2>
<p>Open research is the lifeblood of cumulative progress in science and engineering. Openness is key for fundamental research, for fostering critical computational literacy, and for making informed choices for or against deployment of instruction-tuned LLM architectures. The closed &amp; proprietary nature of ChatGPT and kin makes them fundamentally unfit for responsible use in research and education.</p>
<p>Open alternatives provide ways to build reproducible workflows, chart resource costs, and lessen reliance on corporate whims. One aim of our work here is to provide tools to track openness, transparency and accountability in the fast-evolving landscape of instruction-tuned text generators. Read more in the <a href="https://dl.acm.org/doi/10.1145/3571884.3604316" target="_blank">paper</a> (<a href="https://pure.mpg.de/pubman/item/item_3526897_1/component/file_3526898/Liesenfeld%20et%20al_2023_Opening%20up%20ChatGPT.pdf" target="_blank">PDF</a>) or <a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/" target="_blank">contribute to the repo</a>.</p>
<p class="highlight" id="contribute">If you know a model that should be listed here or a data point that needs updating, please see <a href="https://github.com/opening-up-chatgpt/">guidelines for contributors</a> and feel free to <a href="https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/issues">open an issue or pull request</a> in our repository.</p>
<h2>TL;DR</h2>
<p>Our paper makes the following contributions:</p>
<ul>
<li>We review the risks of relying on proprietary software</li>
<li>We review best practices for open, transparent and accountable 'AI'</li>
<li>We find over 40 ChatGPT alternatives at varying degrees of openness, development and documentation</li>
<li>We argue that tech is never a <em>fait accompli</em> unless we make it so, and that openness enables critical computational literacy</li>
</ul>
<p>We find the following recurrent patterns:</p>
<ul>
<li>Many projects inherit data of dubious legality</li>
<li>Few projects share the all-important instruction-tuning</li>
<li>Preprints are rare, peer-reviewed papers even rarer</li>
<li>Synthetic instruction-tuning data is on the rise, with unknown consequences that are in need of research</li>
</ul>
<p>We conclude as follows:</p>
<blockquote id="conclusion">Openness is not the full solution to the scientific and ethical challenges of conversational text generators. Open data will not mitigate the harmful consequences of thoughtless deployment of large language models, nor the questionable copyright implications of scraping all publicly available data from the internet. However, openness does make original research possible, including efforts to build reproducible workflows and understand the fundamentals of instruction-tuned LLM architectures. Openness also enables checks and balances, fostering a culture of accountability for data and its curation, and for models and their deployment. We hope that our work provides a small step in this direction.
  </blockquote>

-->
<!-- 
<h2>Papers</h2>  
<p>Liesenfeld, Andreas, Alianda Lopez, and Mark Dingemanse. 2023. â€œOpening up ChatGPT: Tracking Openness, Transparency, and Accountability in Instruction-Tuned Text Generators.â€ In <em>CUI '23: Proceedings of the 5th International Conference on Conversational User Interfaces</em>. July 19-21, Eindhoven. doi: <a href="https://doi.org/10.1145/3571884.3604316" target="_blank">10.1145/3571884.3604316</a> (<a href="https://pure.mpg.de/pubman/item/item_3526897_1/component/file_3526898/Liesenfeld%20et%20al_2023_Opening%20up%20ChatGPT.pdf" target="_blank">PDF</a>).</p>

<p>Andreas Liesenfeld and Mark Dingemanse. 2024. Rethinking open source generative AI: open washing and the EU AI Act. In <em>The 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT '24)</em>. Association for Computing Machinery, New York, NY, USA, 1774â€“1787. doi: <a href="https://doi.org/10.1145/3630106.3659005" target="_blank">10.1145/3630106.3659005</a></p>
-->
</div><!-- #content -->
<div id="footer">
<p>This site is an adapted version of <a href="https://opening-up-chatgpt.github.io/">https://opening-up-chatgpt.github.io/</a>.</p>
<p class="copyright">Website &amp; code Â© 2025 by the authors.</p>
<p id="build-time">Table last built on 2025-01-21 at 20:10 UTC</p>
</div>
</body>
</html>
